# Создание LXC контейнеров и Установка etcd

<details>
<summary>ip -c a s</summary>

```bash
root@pmx5:~# ip -c a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master vmbr0 state UP group default qlen 1000
    link/ether 74:56:3c:40:a6:3f brd ff:ff:ff:ff:ff:ff
3: vxlan_dmznet: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master dmznet state UNKNOWN group default qlen 1000
    link/ether d6:ff:b1:9d:3d:5c brd ff:ff:ff:ff:ff:ff
4: dmznet: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether d6:ff:b1:9d:3d:5c brd ff:ff:ff:ff:ff:ff
    inet6 fe80::d4ff:b1ff:fe9d:3d5c/64 scope link 
       valid_lft forever preferred_lft forever
5: vxlan_pgnet: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master pgnet state UNKNOWN group default qlen 1000
    link/ether fa:72:12:dd:ba:d2 brd ff:ff:ff:ff:ff:ff
6: pgnet: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether fa:72:12:dd:ba:d2 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::f872:12ff:fedd:bad2/64 scope link 
       valid_lft forever preferred_lft forever
7: vmbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 74:56:3c:40:a6:3f brd ff:ff:ff:ff:ff:ff
    inet 192.168.87.20/24 scope global vmbr0
       valid_lft forever preferred_lft forever
    inet6 fe80::7656:3cff:fe40:a63f/64 scope link 
       valid_lft forever preferred_lft forever
8: wlp3s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether f0:a6:54:c5:22:47 brd ff:ff:ff:ff:ff:ff
13: tap138i0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master fwbr138i0 state UNKNOWN group default qlen 1000
    link/ether a6:c5:8f:d1:14:fb brd ff:ff:ff:ff:ff:ff
******
67: fwpr157p0@fwln157i0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master vmbr0 state UP group default qlen 1000
    link/ether be:27:3c:43:53:29 brd ff:ff:ff:ff:ff:ff
68: fwln157i0@fwpr157p0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master fwbr157i0 state UP group default qlen 1000
    link/ether ce:13:6e:15:32:4a brd ff:ff:ff:ff:ff:ff
```

</details> 
<br/>


Для создания трех нод в сети `pgnet` с установкой Debian 12, etcd и Patroni, выполните следующие шаги:

### 1. Создаем три LXC контейнера (по одному на каждую ноду)

<details>
<summary>Длинный пример</summary>
    
**Нода 1:**
```bash
pct create 201 local:vztmpl/debian-12-standard_12.2-1_amd64.tar.zst \
  --hostname pg-node1 \
  --cores 4 \
  --memory 4096 \
  --swap 2048 \
  --rootfs /stg/8tb:30 \      # Прямое указание пути
  --storage local \
  --net0 name=eth0,bridge=pgnet,ip=10.10.10.1/24,gw=10.10.10.1 \
  --unprivileged 1 \
  --start
```

**Нода 2:**
```bash
pct create 202 local:vztmpl/debian-12-standard_12.2-1_amd64.tar.zst \
  --hostname pg-node2 \
  --cores 4 \
  --memory 4096 \
  --swap 2048 \
  --rootfs /stg/8tb:30 \      # Прямое указание пути
  --storage local \
  --net0 name=eth0,bridge=pgnet,ip=10.10.10.2/24,gw=10.10.10.1 \
  --unprivileged 1 \
  --start
```

**Нода 3:**
```bash
pct create 203 local:vztmpl/debian-12-standard_12.2-1_amd64.tar.zst \
  --hostname pg-node3 \
  --cores 4 \
  --memory 4096 \
  --swap 2048 \
  --rootfs /stg/8tb:30 \      # Прямое указание пути
  --storage local \
  --net0 name=eth0,bridge=pgnet,ip=10.10.10.3/24,gw=10.10.10.1 \
  --unprivileged 1 \
  --start
```
</details>

### 1. [ALT] Создание виртуальных машин (LXC контейнеров)
Создадим три LXC контейнера с Debian 12 в сети `pgnet`:

```bash
for i in {1..3}; do
  pct create $((200+i)) \
    local:vztmpl/debian-12-standard_12.2-1_amd64.tar.zst \
    --hostname pg-node${i} \
    --cores 4 \
    --memory 4096 \
    --swap 2048 \
    --rootfs /stg/8tb:30 \      # Прямое указание пути
    --storage local \           # Используем прямое хранилище
    --net0 name=eth0,bridge=pgnet,ip=10.10.10.${i}/24,gw=10.10.10.1 \
    --unprivileged 1 \
    --start
done
```

### 1. [ALT2] Создание виртуальных машин (GUI вариант)
Через GUI ProxMox создаём три машины (виртуальные ноды в интерфейсе pgnet (ProxMox)) в рамках доступности нашего кластера:
- ***`192.168.45.201`*** - *pg1* на pmx5;
- ***`192.168.45.202`*** - *pg2* на pmx6;
- ***`192.168.45.204`*** - *pg3* на prox4;

 Корректно будет создать машину `pg1` на `pmx5`, настроить для неё маршрутизацию, включая ***nat*** для ***iptables***, а после - склонировать её также в pmx5 с другими именами и мигрировать, не забыв поменять IP адреса.
<br/> *Маршрутизацию читай в статье* **[System engineering/14. ProxMox: маршрутизация](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/14.%20ProxMox%3A%20маршрутизация.md)**

### 2. Настройка каждой ноды
Зайдите в каждую ноду (`pct enter 201`, `pct enter 202`, `pct enter 203`) и выполните:

#### Обновление системы:
```bash
apt update && apt upgrade -y
```

#### Установка зависимостей:
```bash
apt install -y sudo curl wget gnupg2 software-properties-common
```

### 3. Установка и настройка etcd кластера на Proxmox виртуальных нодах
#### 1. Установка etcd на все ноды

На каждой ноде выполните:

```bash
# Для Ubuntu/Debian
sudo apt update
sudo apt install -y etcd-server

# Для CentOS/RHEL
sudo yum install -y etcd
```
##### 2. Настройка конфигурации etcd
Для каждой ноды нужно создать свой конфигурационный файл. 

##### Пример конфигурации
```bash
 # EXAMPLE:
ETCD_NAME="app"
ETCD_LISTEN_CLIENT_URLS="http://192.168.87.60:2379,http://127.0.0.1:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.87.60:2379,http://127.0.0.1:2379"
ETCD_LISTEN_PEER_URLS="http://192.168.87.60:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.87.60:2380"
ETCD_INITIAL_CLUSTER="app=http://192.168.87.60:2380,app2=http://192.168.87.61:2380,app3=http://192.168.87.62:2380"
```

##### Для pg1 (192.168.45.201):
```bash
sudo tee /etc/default/etcd <<EOF
ETCD_NAME="pg1"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_CLIENT_URLS="http://192.168.45.201:2379,http://127.0.0.1:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.45.201:2379"
ETCD_LISTEN_PEER_URLS="http://192.168.45.201:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.45.201:2380"
ETCD_INITIAL_CLUSTER="pg1=http://192.168.45.201:2380,pg2=http://192.168.45.202:2380,pg3=http://192.168.45.204:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```
##### Для pg2 (192.168.45.202):
```bash
sudo tee /etc/default/etcd <<EOF
ETCD_NAME="pg2"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_CLIENT_URLS="http://192.168.45.202:2379,http://127.0.0.1:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.45.202:2379"
ETCD_LISTEN_PEER_URLS="http://192.168.45.202:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.45.202:2380"
ETCD_INITIAL_CLUSTER="pg1=http://192.168.45.201:2380,pg2=http://192.168.45.202:2380,pg3=http://192.168.45.204:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```

##### Для pg3 (192.168.45.204):
```bash
### /etc/default/etcd 
ETCD_NAME="pg3"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_CLIENT_URLS="http://192.168.45.204:2379,http://127.0.0.1:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.45.204:2379"
ETCD_LISTEN_PEER_URLS="http://192.168.45.204:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.45.204:2380"
ETCD_INITIAL_CLUSTER="pg1=http://192.168.45.201:2380,pg2=http://192.168.45.202:2380,pg3=http://192.168.45.204:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"  # опционально, можно не доавблять
ETCD_INITIAL_CLUSTER_STATE="new"
```
удалить всё из директории `/var/lib/etcd/*`, выполнить `systemctl daemon-reload`.

*Читай подробнее* **[System engineering/ProxMox: "etcd" conf file](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/ProxMox%3A%20%22etcd%22%20conf%20file.md)**

#### Настройка etcd (пример для pg1) старый пример:
```bash
cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd service
Documentation=https://github.com/etcd-io/etcd

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name pg-node1 \\
  --data-dir /var/lib/etcd \\
  --initial-advertise-peer-urls http://10.10.10.1:2380 \\
  --listen-peer-urls http://0.0.0.0:2380 \\
  --listen-client-urls http://0.0.0.0:2379 \\
  --advertise-client-urls http://10.10.10.1:2379 \\
  --initial-cluster-token etcd-cluster \\
  --initial-cluster pg1=http://10.10.10.1:2380,pg2=http://10.10.10.2:2380,pg3=http://10.10.10.3:2380 \\
  --initial-cluster-state new \\
  --heartbeat-interval 1000 \\
  --election-timeout 5000
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

```bash
ETCD_VER=v3.5.0
wget https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xvf etcd-${ETCD_VER}-linux-amd64.tar.gz
cd etcd-${ETCD_VER}-linux-amd64
sudo mv etcd etcdctl /usr/local/bin/
```


Аналогично настройте для pg2 и pg3, изменив IP-адреса и имена.

#### Запуск etcd:
На всех нодах выполните:
```bash
systemctl daemon-reload
systemctl enable --now etcd
sudo systemctl enable etcd
sudo systemctl start etcd
```
#### Проверка работы кластера:
На любой из нод выполнить:
```bash
etcdctl --endpoints=http://192.168.45.201:2379,http://192.168.45.202:2379,http://192.168.45.204:2379 member list
```
#### Сетевые настройки:
   - Убедитесь, что ноды могут общаться между собой по портам 2379 (клиентский) и 2380 (пиринговый)
   - Проверьте, что firewall разрешает эти соединения



# Установка PostgreSQL и Patroni с etcd в качестве DCS

Для настройки отказоустойчивого кластера PostgreSQL с Patroni и etcd выполните следующие шаги:

## 1. Установка PostgreSQL на все ноды

На каждой ноде (pg1, pg2, pg3):

```bash
# Для Ubuntu/Debian
sudo apt update
sudo apt install -y postgresql-15 postgresql-client-15 postgresql-contrib-15

# Для CentOS/RHEL
sudo yum install -y postgresql15-server postgresql15-contrib
```

## 2. Установка Patroni и зависимостей

```bash
# Установка Python и pip
sudo apt install -y python3 python3-pip python3-psycopg2

# Установка Patroni и etcd client
sudo pip3 install patroni[etcd] python-etcd
```

## 3. Настройка Patroni

Создайте конфигурационный файл Patroni на каждой ноде (`/etc/patroni.yml`):

### Для pg1 (192.168.45.201):
```yaml
scope: pg_cluster
namespace: /service/
name: pg1

restapi:
  listen: 192.168.45.201:8008
  connect_address: 192.168.45.201:8008

etcd:
  hosts: ["192.168.45.201:2379", "192.168.45.202:2379", "192.168.45.204:2379"]

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        max_connections: 100
        shared_buffers: 1GB
        dynamic_shared_memory_type: posix
        wal_level: logical
        wal_log_hints: "on"
        archive_mode: "on"
        archive_timeout: 1800s
        archive_command: "/bin/true"
        max_wal_senders: 10
        max_replication_slots: 10
        hot_standby: "on"
        wal_keep_size: 1024MB
        synchronous_commit: "on"
        synchronous_standby_names: "1 (pg2,pg3)"

  initdb:
  - encoding: UTF8
  - data-checksums

  pg_hba:
  - host replication replicator 192.168.45.0/24 md5
  - host all all 192.168.45.0/24 md5

postgresql:
  listen: 192.168.45.201:5432
  connect_address: 192.168.45.201:5432
  data_dir: /var/lib/postgresql/15/main
  bin_dir: /usr/lib/postgresql/15/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: securepassword
    superuser:
      username: postgres
      password: securepassword
  parameters:
    unix_socket_directories: '/var/run/postgresql'

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```

### Для pg2 (192.168.45.202) и pg3 (192.168.45.204):
Аналогично, но измените:
- `name` на `pg2`/`pg3`
- Все IP-адреса на соответствующие (192.168.45.202 или 192.168.45.204)

## 4. Настройка systemd для Patroni

Создайте файл сервиса `/etc/systemd/system/patroni.service`:

```ini
[Unit]
Description=Runners to orchestrate a high-availability PostgreSQL
After=syslog.target network.target

[Service]
Type=simple
User=postgres
Group=postgres
ExecStart=/usr/local/bin/patroni /etc/patroni.yml
KillMode=process
TimeoutSec=30
Restart=no

[Install]
WantedBy=multi-user.target
```

Обновите systemd и запустите Patroni:

```bash
sudo systemctl daemon-reload
sudo systemctl enable patroni
sudo systemctl start patroni
```

## 5. Инициализация кластера

На главной ноде (например, pg1) выполните:

```bash
sudo -u postgres patronictl -c /etc/patroni.yml init pg_cluster
```

## 6. Проверка состояния кластера

```bash
patronictl -c /etc/patroni.yml list
```

Пример вывода:
```
+---------+-----------+------------------+--------+---------+----+-----------+
| Cluster |  Member   |       Host       |  Role  |  State  | TL | Lag in MB |
+---------+-----------+------------------+--------+---------+----+-----------+
| pg_cluster |  pg1     | 192.168.45.201  | Leader | running |  1 |           |
| pg_cluster |  pg2     | 192.168.45.202  |        | running |  1 |         0 |
| pg_cluster |  pg3     | 192.168.45.204  |        | running |  1 |         0 |
+---------+-----------+------------------+--------+---------+----+-----------+
```

## 7. Настройка мониторинга и управления

Установите дополнительные инструменты:

```bash
sudo apt install -y pgbouncer pgadmin4
```

## Важные моменты:

1. **Безопасность**:
   - Измените пароли в конфигурации
   - Настройте брандмауэр для ограничения доступа
   - Рассмотрите возможность использования TLS для соединений

2. **Резервное копирование**:
   - Настройте регулярные резервные копии с помощью pg_basebackup или barman

3. **Мониторинг**:
   - Настройте мониторинг с помощью Prometheus + Grafana
   - Используйте pg_stat_statements для анализа производительности

4. **Обслуживание**:
   - Регулярно выполняйте vacuum и analyze
   - Мониторьте репликацию и лаги

5. **Обновление**:
   - Тестируйте обновления на staging среде
   - Выполняйте обновления в порядке secondary -> primary

Для проверки работы failover можно выполнить:

```bash
# На primary ноде
sudo systemctl stop postgresql

# Проверить автоматический failover
patronictl -c /etc/patroni.yml list
```


