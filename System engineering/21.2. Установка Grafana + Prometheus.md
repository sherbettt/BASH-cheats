Учитывая предыдущий шаг, когда у нас уже имеется подготовленная конфигурация `Grafana + Loki + LogCLI + Promtail`, требуется изменить конфиг на `Grafana + Prometheus`.

Ссылки с подсказками:
1. [Официальная документация Grafana для Prometheus](https://grafana.com/docs/grafana/latest/datasources/prometheus/)
2. [Руководство по установке Prometheus на Linux](https://www.dmosk.ru/instruktions.php?object=prometheus-linux)
3. [Создание графиков в Grafana на основе данных из Prometheus](https://www.dmosk.ru/miniinstruktions.php?mini=grafana-prometheus)
4. [Настройка Prometheus и Grafana](https://redos.red-soft.ru/base/redos-7_3/7_3-administation/7_3-monitoring/7_3-prometheus-grafana/)
5. [Node Exporter Full](https://grafana.com/grafana/dashboards/1860-node-exporter-full/)



# Установка и настройка Prometheus в существующую систему Grafana + Loki

## Текущая система (работает, не трогать):
- ✅ **Grafana** (порт 3000) - визуализация
- ✅ **Loki** (порт 3100) - логирование (оставить)
- ✅ **Promtail** - сбор логов (оставить)
- ✅ **LogCLI** - CLI для запросов к Loki

## Цель:
Добавить **Prometheus** для сбора метрик, сохранив существующую систему логирования.
<br/>

## Интерфейсы
```bash
root@runelfrontdev /etc/prometheus
11:00:35 # ipa
lo               UNKNOWN        127.0.0.1/8 ::1/128 
enp3s0           UP             192.168.87.179/24 fe80::365a:60ff:fee5:393b/64 
zt7nnkx5rb       UNKNOWN        192.168.195.238/24 fe80::b8cd:d3ff:fe36:5b99/64 

root@starodubtcevs:~# ipa
lo               UNKNOWN        127.0.0.1/8 ::1/128 
ens3             UP             91.207.75.89/32 fe80::5054:ff:fe26:e918/64 
zt7nnkx5rb       UNKNOWN        192.168.195.122/24 fe80::b817:30ff:fe36:5c32/64 
```


## Шаг 1: Установка Prometheus на runelfrontdev

### Установите пакеты:
```bash
sudo apt update
sudo apt install -y prometheus prometheus-alertmanager prometheus-bind-exporter prometheus-nginx-exporter
```

### Проверьте установку:
```bash
prometheus --version
prometheus-node-exporter --version

# Создайте пользователя (если не создан)
sudo useradd --no-create-home --shell /bin/false prometheus 2>/dev/null || true
```
<br/>



## Шаг 2: Установка Node-exporter на все серверы

### На runelfrontdev (локальный):
```bash
sudo apt install -y prometheus-node-exporter prometheus-node-exporter-collectors
sudo systemctl enable --now prometheus-node-exporter
```

### Проверка на runelfrontdev:
```bash
prometheus-node-exporter --version
systemctl status prometheus prometheus-node-exporter --no-pager
sudo ss -tulpn | grep -E ':9090|:9100'
```

### На starodubtcevs (удалённый):
```bash
# Подключитесь к удалённому серверу
ssh root@starodubtcevs.hlab.kz "
sudo apt update
sudo apt install -y prometheus-node-exporter
sudo systemctl enable --now prometheus-node-exporter

# Проверка
prometheus-node-exporter --version
systemctl status prometheus-node-exporter --no-pager
sudo ss -tulpn | grep -E ':9090|:9100'
"
```
<br/>


## Шаг 3: Настройка конфигурации Prometheus

### Создайте backup текущего конфига:
```bash
sudo cp /etc/prometheus/prometheus.yml /etc/prometheus/prometheus.yml.backup
```

### Установите новую конфигурацию:
**`scrape_timeout`** должен быть строго меньше **`scrape_interval`**


<details>
<summary>❗ prometheus.yml - общий вид ❗</summary>

```yaml
# /etc/prometheus/prometheus.yml
# Конфигурационный файл Prometheus для мониторинга инфраструктуры

# ================= ГЛОБАЛЬНЫЕ НАСТРОЙКИ =================
global:
  # Интервал сбора метрик со всех целевых серверов
  # Каждые 15 секунд Prometheus будет опрашивать все targets
  scrape_interval: 15s
  
  # Интервал пересчёта правил для алертов и записей
  # Каждые 15 секунд проверяются alerting и recording rules
  evaluation_interval: 15s
  
  # Опциональные настройки (раскомментировать при необходимости):
  # external_labels:                   # Метки для всех time series
  #   cluster: 'prod'                  # Идентификатор кластера
  #   region: 'eu-west'                # Регион размещения

# ================= НАСТРОЙКИ ALERTMANAGER =================
# Конфигурация для отправки алертов в Alertmanager
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # Разкомментируйте и настройте при установке Alertmanager:
          # - localhost:9093          # Alertmanager по умолчанию слушает порт 9093
          
# ================= ФАЙЛЫ С ПРАВИЛАМИ =================
# Подключаемые файлы с правилами для алертов и записи метрик
rule_files:
  # Примеры файлов с правилами (создайте при необходимости):
  # - "alerts/node_rules.yml"        # Правила для node-exporter
  # - "alerts/prometheus_rules.yml"  # Правила для самого Prometheus
  # - "recording_rules.yml"          # Правила для предрасчёта метрик

# ================= КОНФИГУРАЦИИ СБОРА МЕТРИК =================
# Основной раздел - определяет какие сервисы и как мониторить

scrape_configs:
  # ================= 1. САМ PROMETHEUS =================
  # Мониторинг самого Prometheus (самомониторинг)
  - job_name: 'prometheus'
    # Статическая конфигурация целей
    static_configs:
      - targets: ['localhost:9090']  # Prometheus слушает на порту 9090
    # Дополнительные метки для этой группы целей
    labels:
      component: 'monitoring'        # Тип компонента
      service: 'prometheus'          # Имя сервиса
      env: 'production'              # Окружение

  # ================= 2. ЛОКАЛЬНЫЙ NODE-EXPORTER =================
  # Сбор системных метрик с локального сервера (runelfrontdev)
  - job_name: 'node-exporter-runelfrontdev'
    static_configs:
      - targets: ['localhost:9100']  # Node-exporter слушает на порту 9100
    labels:
      host: 'runelfrontdev'          # Имя хоста
      location: 'local'              # Локация (локальный сервер)
      component: 'infrastructure'    # Тип компонента
      service: 'node-exporter'       # Имя сервиса
      env: 'production'              # Окружение
    # Опциональные настройки:
    # scrape_timeout: 10s            # Таймаут сбора метрик (по умолчанию 10s)
    # scrape_interval: 15s           # Можно переопределить глобальный interval

  # ================= 3. УДАЛЁННЫЙ NODE-EXPORTER =================
  # Сбор системных метрик с удалённого сервера (starodubtcevs)
  - job_name: 'node-exporter-starodubtcevs'
    static_configs:
      # Используем ZeroTier IP для подключения к удалённому серверу
      - targets: ['192.168.195.122:9100']
    labels:
      host: 'starodubtcevs'          # Имя удалённого хоста
      location: 'remote'             # Локация (удалённый сервер)
      component: 'infrastructure'    # Тип компонента
      service: 'node-exporter'       # Имя сервиса
      env: 'production'              # Окружение
    # Увеличенный таймаут для удалённых подключений
    scrape_timeout: 30s
    # Дополнительные настройки для проблемных сетей:
    # Для медленных соединений можно увеличить:
    # scrape_interval: 30s            # Увеличение интервала опроса
    
    # Если нужна работа через прокси:
    # proxy_url: http://proxy.example.com:8080
    
    # Настройки TLS (если используется HTTPS):
    # tls_config:
    #   insecure_skip_verify: true   # Пропустить проверку сертификата (небезопасно!)
    #   ca_file: /path/to/ca.crt     # Путь к CA сертификату

  # ================= 4. МЕТРИКИ LOKI =================
  # Сбор внутренних метрик Loki (логирование)
  - job_name: 'loki-metrics'
    static_configs:
      - targets: ['localhost:3100']  # Loki слушает на порту 3100
    # Loki предоставляет метрики по пути /metrics
    metrics_path: '/metrics'
    labels:
      component: 'logging'           # Тип компонента (система логирования)
      service: 'loki'                # Имя сервиса
      env: 'production'              # Окружение
    # Увеличен интервал сбора, т.к. метрики логирования меняются медленнее
    scrape_interval: 30s

  # ================= 5. МЕТРИКИ PROMTAIL =================
  # Сбор метрик сборщика логов Promtail
  - job_name: 'promtail-metrics'
    static_configs:
      - targets: ['localhost:9080']  # Promtail слушает на порту 9080
    metrics_path: '/metrics'
    labels:
      component: 'logging'           # Тип компонента
      service: 'promtail'            # Имя сервиса
      env: 'production'              # Окружение
    scrape_interval: 30s

  # ================= 6. МЕТРИКИ GRAFANA =================
  # Сбор внутренних метрик Grafana
  - job_name: 'grafana-metrics'
    static_configs:
      - targets: ['localhost:3000']  # Grafana слушает на порту 3000
    metrics_path: '/metrics'
    scheme: 'http'                   # Протокол (http/https)
    labels:
      component: 'monitoring'        # Тип компонента
      service: 'grafana'             # Имя сервиса
      env: 'production'              # Окружение
    scrape_interval: 30s

  # ================= 7. BLACKBOX EXPORTER (ОПЦИОНАЛЬНО) =================
  # Для мониторинга доступности сервисов (HTTP/HTTPS/TCP/ICMP)
  # Установите blackbox_exporter: https://github.com/prometheus/blackbox_exporter
  # - job_name: 'blackbox-http'
  #   metrics_path: /probe
  #   params:
  #     module: [http_2xx]           # Модуль проверки HTTP 200 OK
  #   static_configs:
  #     - targets:
  #       - http://localhost:3000    # Проверка доступности Grafana
  #       - http://localhost:9090    # Проверка доступности Prometheus
  #       - http://localhost:3100    # Проверка доступности Loki
  #   relabel_configs:
  #     - source_labels: [__address__]
  #       target_label: __param_target
  #     - source_labels: [__param_target]
  #       target_label: instance
  #     - target_label: __address__
  #       replacement: localhost:9115  # Адрес blackbox_exporter

  # ================= 8. CADVISOR (ОПЦИОНАЛЬНО) =================
  # Для мониторинга контейнеров Docker
  # - job_name: 'cadvisor'
  #   static_configs:
  #     - targets: ['localhost:8080']  # cAdvisor порт по умолчанию
  #   labels:
  #     component: 'containers'
  #     service: 'cadvisor'

  # ================= 9. POSTGRES EXPORTER (ОПЦИОНАЛЬНО) =================
  # Для мониторинга баз данных PostgreSQL
  # - job_name: 'postgres-exporter'
  #   static_configs:
  #     - targets: ['localhost:9187']  # PostgreSQL exporter порт
  #   labels:
  #     component: 'database'
  #     service: 'postgres'

  # ================= 10. NGINX EXPORTER (ОПЦИОНАЛЬНО) =================
  # Для мониторинга Nginx
  # - job_name: 'nginx-exporter'
  #   static_configs:
  #     - targets: ['localhost:9113']  # Nginx exporter порт
  #   labels:
  #     component: 'web-server'
  #     service: 'nginx'

# ================= ПРИМЕЧАНИЯ ПО БЕЗОПАСНОСТИ =================
# 1. Ограничьте доступ к портам:
#    - 9090 (Prometheus) - только для внутреннего использования
#    - 9100 (Node-exporter) - только для Prometheus
#
# 2. Рассмотрите использование:
#    - Basic Auth: https://prometheus.io/docs/guides/basic-auth/
#    - TLS: https://prometheus.io/docs/prometheus/latest/configuration/https/
#    - Reverse Proxy (nginx/apache) с аутентификацией
#
# 3. Для продакшена рекомендуется:
#    - Вынести конфигурацию в отдельные файлы
#    - Использовать service discovery вместо static_configs
#    - Настроить Alertmanager для уведомлений
```
</details>

=-=-=-=-=-=-=-=
<details>
<summary>❗ prometheus.yml ❗</summary>

```yaml
# /etc/prometheus/prometheus.yml
# Конфигурация для мониторинга двух серверов через ZeroTier

global:
  scrape_interval: 30s      # Каждые 30 секунд собираем метрики
  evaluation_interval: 30s  # Каждые 30 секунд проверяем правила

# Настройки алертов (пока отключены)
alerting:
  alertmanagers:
    - static_configs:
        - targets: []
          # - alertmanager:9093  # Раскомментировать при установке Alertmanager

# Файлы с правилами (пока пусто)
rule_files:
  # - "alert_rules.yml"

# Конфигурации сбора метрик
scrape_configs:
  # =============== 1. САМ PROMETHEUS ===============
  # Мониторинг самого Prometheus
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
        labels:
          host: 'runelfrontdev'
          service: 'prometheus'
          component: 'monitoring'
          env: 'production'

  # =============== 2. ЛОКАЛЬНЫЙ СЕРВЕР (runelfrontdev) ===============
  # Node-exporter на локальном сервере
  - job_name: 'node-local'
    static_configs:
      - targets: ['localhost:9100']
        labels:
          host: 'runelfrontdev'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'local'
          env: 'production'

  # =============== 3. УДАЛЁННЫЙ СЕРВЕР (starodubtcevs) ===============
  # Node-exporter на удалённом сервере через ZeroTier
  - job_name: 'node-remote'
    static_configs:
      - targets: ['192.168.195.122:9100']
        labels:
          host: 'starodubtcevs'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'remote'
          env: 'production'
    scrape_timeout: 25s  # Увеличенный таймаут для удалённого сервера

  # =============== 4. LOKI (лог-система) ===============
  # Метрики системы логирования Loki
  - job_name: 'loki'
    static_configs:
      - targets: ['localhost:3100']
        labels:
          service: 'loki'
          component: 'logging'
    metrics_path: '/metrics'  # Loki предоставляет метрики по этому пути
    scrape_interval: 30s      # Метрики логирования меняются реже

  # =============== 5. PROMTAIL ===============
  # Метрики сборщика логов Promtail
  - job_name: 'promtail'
    static_configs:
      - targets: ['localhost:9080']
        labels:
          service: 'promtail'
          component: 'logging'
    metrics_path: '/metrics'
    scrape_interval: 30s

  # =============== 6. GRAFANA ===============
  # Метрики Grafana (опционально)
  - job_name: 'grafana'
    static_configs:
      - targets: ['localhost:3000']
        labels:
          service: 'grafana'
          component: 'monitoring'
    metrics_path: '/metrics'
    scrape_interval: 30s
```
</details>

=-=-=-=-=-=-=-=
<details>
<summary>❗ prometheus.yml - для трёх машин ❗</summary>

```yaml
# /etc/prometheus/prometheus.yml
# Конфигурация для мониторинга серверов через WireGuard

global:
  scrape_interval: 30s      # Каждые 30 секунд собираем метрики
  evaluation_interval: 30s  # Каждые 30 секунд проверяем правила

# Настройки алертов (пока отключены)
alerting:
  alertmanagers:
    - static_configs:
        - targets: []
          # - alertmanager:9093  # Раскомментировать при установке Alertmanager

# Файлы с правилами (пока пусто)
rule_files:
  # - "alert_rules.yml"

# Конфигурации сбора метрик
scrape_configs:
  # =============== 1. САМ PROMETHEUS ===============
  # Мониторинг самого Prometheus
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
        labels:
          host: 'runelfrontdev'
          service: 'prometheus'
          component: 'monitoring'
          env: 'production'

  # =============== 2. ЛОКАЛЬНЫЙ СЕРВЕР (runelfrontdev) ===============
  # Node-exporter на локальном сервере
  - job_name: 'grafana-local'
    static_configs:
      - targets: ['localhost:9100']
        labels:
          host: 'runelfrontdev'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'local'
          env: 'production'

  # =============== 3. УДАЛЁННЫЕ СЕРВЕРА msk-vps* ===============
  # Один job для всех удаленных серверов - это правильный подход
  - job_name: 'vps-remote'
    static_configs:
      - targets: 
        - '172.16.15.36:9100'  # msk-vps1
        - '172.16.15.37:9100'  # msk-vps2
        - '172.16.15.38:9100'  # msk-vps3
        labels:
          location: 'remote'
          env: 'production'
    scrape_timeout: 25s
    relabel_configs:
      # Добавляем метку host на основе IP адреса
      - source_labels: [__address__]
        target_label: host
        regex: '172\.16\.15\.(\d+):9100'
        replacement: 'msk-vps$1'
      - source_labels: [__address__]
        target_label: service
        replacement: 'node-exporter'
      - source_labels: [__address__]
        target_label: component
        replacement: 'infrastructure'

# Можно изменить regex для получения более красивых имен:
#relabel_configs:
#  - source_labels: [__address__]
#    target_label: host
#    regex: '172\.16\.15\.(36|37|38):9100'
#    replacement: 'msk-vps-$1'
  # или даже так:
#  - source_labels: [__address__]
#    target_label: host
#    regex: '172\.16\.15\.36:9100'
#    replacement: 'web-server-1'
#  - source_labels: [__address__]
#    target_label: host  
#    regex: '172\.16\.15\.37:9100'
#    replacement: 'db-server'
#  - source_labels: [__address__]
#    target_label: host
#    regex: '172\.16\.15\.38:9100'
#    replacement: 'app-server'

  # =============== 4. LOKI (лог-система) ===============
  # Метрики системы логирования Loki
  - job_name: 'loki'
    static_configs:
      - targets: ['localhost:3100']
        labels:
          host: 'runelfrontdev'
          service: 'loki'
          component: 'logging'
          env: 'production'
    metrics_path: '/metrics'
    scrape_interval: 30s

  # =============== 5. PROMTAIL ===============
  # Метрики сборщика логов Promtail
  - job_name: 'promtail'
    static_configs:
      - targets: ['localhost:9080']
        labels:
          host: 'runelfrontdev'
          service: 'promtail'
          component: 'logging'
          env: 'production'
    metrics_path: '/metrics'
    scrape_interval: 30s

  # =============== 6. GRAFANA ===============
  # Метрики Grafana (опционально)
  - job_name: 'grafana'
    static_configs:
      - targets: ['localhost:3000']
        labels:
          host: 'runelfrontdev'
          service: 'grafana'
          component: 'monitoring'
          env: 'production'
    metrics_path: '/metrics'
    scrape_interval: 30s
```
</details>

=-=-=-=-=-=-=-=
<details>
<summary>❗ отдельные job для каждого сервера ❗</summary>

```yaml
  # Вариант с отдельными job для каждого сервера
  - job_name: 'node-msk-vps1'
    static_configs:
      - targets: ['172.16.15.36:9100']
        labels:
          host: 'msk-vps1'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'remote'
          env: 'production'
    scrape_timeout: 25s

  - job_name: 'node-msk-vps2'
    static_configs:
      - targets: ['172.16.15.37:9100']
        labels:
          host: 'msk-vps2'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'remote'
          env: 'production'
    scrape_timeout: 25s

  - job_name: 'node-msk-vps3'
    static_configs:
      - targets: ['172.16.15.38:9100']
        labels:
          host: 'msk-vps3'
          service: 'node-exporter'
          component: 'infrastructure'
          location: 'remote'
          env: 'production'
    scrape_timeout: 25s
```
</details>

### Объяснение регулярки:
1. `source_labels: [__address__]` - берем исходный адрес target (например 172.16.15.36:9100)
2. `regex: '172\.16\.15\.(\d+):9100'` - применяем регулярное выражение
3. `172\.16\.15\.` - ищем этот паттерн
4. `(\d+)` - захватываем последний октет IP (36, 37, 38)
5. `replacement: 'msk-vps$1'` - заменяем на `msk-vps` + захваченное число
6. `target_label: host` - сохраняем результат в метку host

Результат для каждого сервера:
- 172.16.15.36:9100 → host="msk-vps36"
- 172.16.15.37:9100 → host="msk-vps37"
- 172.16.15.38:9100 → host="msk-vps38"

### Как использовать в запросах PromQL:
```promql
# Все метрики CPU для всех VPS
node_cpu_seconds_total{job="node-vps"}

# Только для конкретного хоста
node_cpu_seconds_total{host="msk-vps36"}

# Группировка по хосту
sum(rate(node_cpu_seconds_total{job="node-vps", mode="idle"}[5m])) by (host)
```


Проверка синтаксиса:
```bash
promtool check config /etc/prometheus/prometheus.yml
```
<br/>


## Шаг 4: Проверка доступности удалённого node-exporter

### Проверьте подключение:
```bash
curl -s --max-time 5 http://192.168.195.122:9100/metrics | head -5
```

### Если недоступен - настройте firewall на starodubtcevs:
```bash
ssh root@starodubtcevs.hlab.kz "
# Проверьте текущие правила
sudo ufw status
sudo iptables -L -n | grep 9100

# Откройте порт (выберите один вариант)
sudo ufw allow 9100/tcp comment 'Prometheus node-exporter'
# Или
sudo iptables -I INPUT -p tcp --dport 9100 -j ACCEPT
"
```
<br/>


## Шаг 5: Перезапуск и проверка Prometheus

### Перезапустите службу:
```bash
sudo systemctl restart prometheus
```

### Проверьте статус:
```bash
sudo systemctl status prometheus --no-pager
sudo journalctl -u prometheus -n 20 --no-pager
```
<br/>


## Шаг 6: Проверка основных метрик Node Exporter
Зайти по ссылке http://192.168.87.209:9090/classic/graph в Graph
```promql
# Загрузка CPU
100 - (avg by (host) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Использование памяти
node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes

# Использование диска
node_filesystem_size_bytes - node_filesystem_free_bytes

# Количество обновлений (видно из curl)
apt_upgrades_pending
```
<br/>


## Шаг 7: Проверка targets в Prometheus

### Проверка через веб-интерфейс:
- Веб-интерфейс Prometheus (http://192.168.87.179:9090)
- Prometheus Targets: http://192.168.87.179:9100/targets
- Prometheus Targets: http://192.168.87.179:9090/classic/targets
- Веб-интерфей Grafana (http://192.168.87.179:3000)


### Проверьте все цели:
```bash
curl -s http://localhost:9090/api/v1/targets | python3 -m json.tool | grep -A2 -B2 '"health"'

curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[].labels'
```

### Проверка конкретных метрик:
```bash
# Проверка доступности node-exporters
curl -s "http://localhost:9100/metrics" | grep -E "^node_cpu_seconds_total{mode=\"idle\"}" | head -1
curl -s --max-time 5 "http://192.168.195.122:9100/metrics" | grep -E "^node_cpu_seconds_total{mode=\"idle\"}" | head -1

# Проверка метрик Prometheus о себе
curl -s "http://localhost:9090/api/v1/query?query=prometheus_build_info" | python3 -m json.tool | grep -A2 '"metric"'
```

### Или используйте читаемый формат:

<details>
<summary>❗ читаемый формат - скрипты ❗</summary>

```bash
curl -s "http://localhost:9090/api/v1/targets" | python3 -c "
import json, sys
data = json.load(sys.stdin)
print('=== PROMETHEUS TARGETS STATUS ===')
print('Всего целей:', len(data['data']['activeTargets']))
print()
for target in data['data']['activeTargets']:
    health = target['health']
    job = target['labels'].get('job', 'unknown')
    instance = target['scrapeUrl'].replace('http://', '').replace('https://', '')
    
    if health == 'up':
        status = '✅'
    else:
        status = '❌'
    
    print(f'{status} {job:20} {instance:40} - {health}')
    # Покажем последнюю ошибку если есть
    if 'lastError' in target and target['lastError']:
        print(f'    Ошибка: {target[\"lastError\"]}')
"
```

```bash
curl -s http://localhost:9090/api/v1/targets | python3 -c "
import json, sys
data = json.load(sys.stdin)
print('=== PROMETHEUS TARGETS ===')
for target in data['data']['activeTargets']:
    status = '✅' if target['health'] == 'up' else '❌'
    print(f'{status} {target[\"scrapeUrl\"]} - {target[\"health\"]}')
"
```
</details>
<br/>


## Шаг 8: Добавление Prometheus в Grafana

### Проверьте работу Grafana:
```bash
curl -s http://localhost:3000 | head -3
```

### Инструкция по добавлению в UI:
1. Откройте браузер: http://192.168.87.179:3000
2. Логин: `admin/admin` (или ваш пароль)
3. Перейдите: **Connections** → **Data Sources**
4. Нажмите: **Add data source**
5. Выберите: **Prometheus**
6. Настройте:
   - **Name:** `Prometheus`
   - **URL:** `http://localhost:9090`
7. Нажмите: **Save & Test** (должно быть "Data source is working")

### Инструкция по добавлению Через API:
```bash
# Установите переменные (замените пароль!)
GRAFANA_URL="http://localhost:3000"
GRAFANA_USER="admin"
GRAFANA_PASS="ваш_пароль"  # Замените на ваш пароль!

# Получите API key или используйте basic auth
curl -s -X POST -H "Content-Type: application/json" \
  -d '{
    "name": "Prometheus",
    "type": "prometheus",
    "url": "http://localhost:9090",
    "access": "proxy",
    "basicAuth": false,
    "isDefault": false
  }' \
  "${GRAFANA_URL}/api/datasources" \
  --user "${GRAFANA_USER}:${GRAFANA_PASS}" | python3 -m json.tool
```
<br/>



## Шаг 9: Импорт готовых дашбордов

### Вариант A - Импорт через UI:
1. В Grafana: **Dashboards** → **New** → **Import**
2. Введите ID дашборда:
   - `1860` - Node Exporter Full
   - `11074` - 1 Node Exporter for Prometheus
   - `3662` - Prometheus 2.0 Stats
   - `6417` - Grafana Loki Dashboard
3. Выберите Prometheus как источник данных
4. Нажмите **Import**

### Вариант B - Создание простых панелей:
- CPU использование: `100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Память: `node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes`
- Диск: `node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} * 100`
<br/>



## Шаг 10: Комплексная проверка системы

### Создайте тестовый скрипт:
```bash
cat << 'EOF' > /tmp/test_prometheus.sh
#!/bin/bash
echo "=== КОМПЛЕКСНАЯ ПРОВЕРКА PROMETHEUS ==="
echo ""

echo "1. Prometheus API:"
curl -s -o /dev/null -w "HTTP: %{http_code}\n" http://localhost:9090

echo ""
echo "2. Локальный node-exporter:"
curl -s -o /dev/null -w "HTTP: %{http_code}\n" http://localhost:9100/metrics

echo ""
echo "3. Удалённый node-exporter:"
curl -s -o /dev/null -w "HTTP: %{http_code}\n" --max-time 5 http://192.168.195.122:9100/metrics

echo ""
echo "4. Состояние всех targets:"
curl -s http://localhost:9090/api/v1/targets 2>/dev/null | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    targets = data['data']['activeTargets']
    up = sum(1 for t in targets if t['health'] == 'up')
    print(f'Активных: {up}/{len(targets)}')
    for t in targets:
        status = '✅' if t['health'] == 'up' else '❌'
        print(f'  {status} {t[\"scrapeUrl\"]}')
except:
    print('Ошибка получения targets')
"

echo ""
echo "5. Доступные метрики (первые 5):"
curl -s http://localhost:9090/api/v1/label/__name__/values 2>/dev/null | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(f'Всего метрик: {len(data[\"data\"])}')
    for metric in data['data'][:5]:
        print(f'  - {metric}')
except:
    print('Ошибка получения метрик')
"
EOF

chmod +x /tmp/test_prometheus.sh
/tmp/test_prometheus.sh
```
<br/>


## Полезные команды для управления:

### Проверка конфигурации:
```bash
promtool check config /etc/prometheus/prometheus.yml
```

### Просмотр метрик в реальном времени:
```bash
curl -s http://localhost:9090/api/v1/query?query=up | python3 -m json.tool
```

### Проверка конкретных метрик:
```bash
curl -s "http://localhost:9090/api/v1/query?query=node_cpu_seconds_total" | python3 -m json.tool
```

### Веб-интерфейс Prometheus:
```bash
echo "Откройте в браузере: http://192.168.87.179:9090"
```
<br/>


## Шаг 11: Установка Grafana Tempo
Что такое Grafana Tempo?
<br/> **Grafana Tempo** — это система распределенной трассировки (distributed tracing), которая хранит и визуализирует трассировки выполнения запросов в распределенных системах.

Простыми словами:
<br/> Если **Prometheus** отвечает на вопрос "Что сломалось?" **(метрики)**,
<br/> а **Loki** — "Что происходило?" **(логи)**,
<br/> то **Tempo** отвечает на вопрос "Почему это сломалось и как именно?" **(трассировки выполнения)**.

### Качаем с GitHub для Debian дистрибутива
```bash
curl -LO https://github.com/grafana/tempo/releases/download/v2.9.0/tempo_2.9.0_linux_amd64.deb
dpkg -i tempo_2.9.0_linux_amd64.deb
```
```bash
# или
curl -LO https://github.com/grafana/tempo/releases/download/v2.9.0/tempo_2.9.0_linux_amd64.tar.gz

# Распакуйте
tar xzf tempo_2.7.0_linux_amd64.tar.gz

# Установите
sudo mv tempo /usr/local/bin/
sudo mkdir -p /etc/tempo /var/lib/tempo
```
### Проверка
```bash
systemctl status tempo.service -l --no-pager
```
<br/>


## Шаг 12: Конфигурирование Grafana Tempo








