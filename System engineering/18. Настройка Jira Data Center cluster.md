**Читай статью с оф. сайта: [Set up a Jira Data Center cluster](https://confluence.atlassian.com/adminjiraserver/set-up-a-jira-data-center-cluster-993929600.html)**

## 1. Клонирование LXC с Jira

**Через GUI.**
Как всегда заходим в [ProxMox](https://192.168.87.6:8006/#v1:0:=lxc%2F169:4::=contentIso:::8:11:), 
ищем контейнер ***Container 169 (jira-new) on node 'prox4'***, в браузере по F12 определяем  IP адрес Jira (192.168.87.219:443), проверяем с помощью nmap. 
Создаём снепшот и после - клонируем машину, присваивая ей новое имя.
```bash
# Сканирование используемых IP адресов
nmap -A 192.168.46.0/24
nmap -sn 192.168.46.0/24
nmap -sn 192.168.46.0/24 -oG - | grep "Up" | awk '{print $2}'
nmap -sn 192.168.46.0/24 | grep "Nmap scan report" | awk '{print $5}'
```

<details>
<summary>❗ Через pvesh, Через pct ❗</summary>
  
**Через pvesh.**
  <br/> Проверяем на всякий случай:
```bash
# зайти
ssh root@192.168.87.17  # prox4
# утилита и команды
pvesh get /nodes/prox4/lxc/169/config
pvesh get /nodes/prox4/lxc/169/interfaces  # 192.168.87.219/24
pvesh get /nodes/prox4/storage  # проверить все места хранения
```

Создаём снепшот:
```bash
# Создаём там же, где и контейнер, на ssd_1tb
# pvesh
pvesh create /nodes/prox4/lxc/169/snapshot \
    --snapname "before-update" \
    --description "Снэпшот перед обновлением Jira"

# pct
pct snapshot 169 "backup-2024" --description "Резервная копия на 2024 год" --vmstate --live
```

Клонируем:
```bash
pvesh create /nodes/prox4/lxc/169/clone \
    --newid 181 \
    --storage ssd_1tb \
    --hostname jira-cluster
```

**Через pct.**
  <br/> Если `pvesh` не работает, можно использовать `pct` (Proxmox Container Toolkit):
```bash
pct clone 169 181 \
    --storage ssd_1tb \
    --hostname jira-cluster
```
</details>


В результате получаем ***Container 181 (jira-cluster) on node 'prox4'*** c IP = 192.168.87.140/24 (желательно выдать static IP). 
<br/> Т.к. сеть 192.168.87.0/24 предназначена в рамках RunTel для выхода в и-нет, то для созданного контейнера важно поменять интерфейс **`eth0`** на **`192.168.46.0/24`** сеть, выбрав свободный IP, в результате выбираем свободный на текущий момент адрес  **`192.168.46.2`** (Bridge = dmznet). 
<br/> Проверяй нашу машину **[102 (dmzgateway)](https://192.168.87.6:8006/#v1:0:=lxc%2F102:4::::::11:2)** на шлюзы.

Чтобы на новой машине ***jira-cluster*** не было конфликта с почтовыми уведомлениями относительно ***jira-new***, необходимо прописать т.н. "ложный путь" временно, до тех пор пока не будет полностью запущен Jira кластер!
```bash
ip route add 77.88.21.125 via 192.168.87.4 dev eth0;
ip route add 77.88.21.125 via 192.168.46.254 dev eth0
```
<br/>
<br/>


## 2. Сопряжение с S3
**ВАЖНО! все машины используют ядро ProxMox - "6.8.12-9-pve", из-за чего работа `nfs-kernel-server` будет поломана**
Проверка конфигурации контейнеров в pvesh
```
ccat /etc/pve/lxc/181.conf ; ccat /etc/pve/lxc/169.conf
```

Вводные данные:
- **/opt/atlassian/jira/shared** (если используется кластер)
- **/var/atlassian/application-data/jira** (обычно здесь лежат данные)
- **/opt/atlassian/jira/conf/dbconfig.xml**  (конф. файлик для Jira БД PSQL)

### 1. Создать файл `/opt/atlassian/jira/conf/cluster.properties` на машине jira-cluster (192.168.46.2)
```ini
# jira-cluster
jira.node.id=node1
jira.shared.home=/var/atlassian/application-data/jira
#jira.shared.home=/opt/atlassian/jira/shared
jira.lb.enabled=true
jira.lb.https=false
jira.lb.hostname=192.168.87.140  # или DNS имя
jira.lb.port=8080
```

### 2. Создать/проверить файл `/var/atlassian/application-data/jira/filestore-config.xml` на jira-* маншинах
```ini
<?xml version="1.1" ?>
<filestore-config>
  <filestores>
    <s3-filestore id="attachmentBucket">
      <config> 
        <bucket-name>jira-attachments</bucket-name> 
        <region>ru-runtel1</region>
        <endpoint-override>http://192.168.87.242:9000</endpoint-override>
#       <endpoint-override>https://s3.runtel.org</endpoint-override>
#       <endpoint-override>https://s3-2.runtel.org</endpoint-override>
      </config>
    </s3-filestore>
  </filestores>
  <associations>
    <association target="attachments" file-store="attachmentBucket" />
  </associations>
</filestore-config>
```

### 3. Создать файл `/var/atlassian/application-data/jira/hazelcast.properties` на машинах jira-cluster и jira-new
```ini
hazelcast.network.tcpip.members=192.168.87.140,192.168.87.219
hazelcast.group.name=jira-cluster
hazelcast.group.password=securepassword
hazelcast.interface=192.168.87.140 # на первой ноде
# hazelcast.interface=192.168.87.219 # на второй ноде
```

### 4. Логин по адресу.
Зайти на web-интерфейс: **http://192.168.87.140:8080/secure/Dashboard.jspa**
<br/>
<br/>



## 3. Проверка БД PostgreSQL на всех машинах
### 1. Проверить файл конф. БД `/var/atlassian/application-data/jira/dbconfig.xml`
<details>
<summary>❗Jira - dbconfig.xml ❗</summary>

```xml
<?xml version="1.0" encoding="UTF-8"?>

<jira-database-config>
  <name>defaultDS</name>
  <delegator-name>default</delegator-name>
  <database-type>postgres72</database-type>
  <schema-name>public</schema-name>
  <jdbc-datasource>
    <url>jdbc:postgresql://127.0.0.1:5432/jira</url>    # позже изменить на адрес Patroni сервера
    <driver-class>org.postgresql.Driver</driver-class>
    <username>jira</username>
    <password>{ATL_SECURED}</password>
    <pool-min-size>40</pool-min-size>
    <pool-max-size>40</pool-max-size>
    <pool-max-wait>30000</pool-max-wait>
    <validation-query>select 1</validation-query>
    <min-evictable-idle-time-millis>60000</min-evictable-idle-time-millis>
    <time-between-eviction-runs-millis>300000</time-between-eviction-runs-millis>
    <pool-max-idle>40</pool-max-idle>
    <pool-remove-abandoned>true</pool-remove-abandoned>
    <pool-remove-abandoned-timeout>300</pool-remove-abandoned-timeout>
    <pool-test-on-borrow>false</pool-test-on-borrow>
    <pool-test-while-idle>true</pool-test-while-idle>
    <connection-properties>tcpKeepAlive=true</connection-properties>
  </jdbc-datasource>
</jira-database-config>
```
</details>

Потребуется Создать дамп текущей БД и перенести его на машину с Patroni. После переноса и восстановления БД из дампа, ннобходимо будет в строке `<url>jdbc:postgresql://127.0.0.1:5432/jira</url>` указать адрес машины Patroni.
<br/>

### 2. Создать дамп текущей БД и перенести его на машину Patroni
Читай раздел **[Создание дампа БД PostgreSQL](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/16.%20PSQL%20dump%3A%20клон%20%2B%20восстановление.md#-4-создание-дампа-бд-postgresql)**
```bash
# должна быть БД с именем "jira"
sudo -u postgres psql -c "\du" ;
sudo -u postgres psql -c "\l" ;
```
```bash
# Переходим в каталог с бэкапами
cd /var/backups/postgres
chown -R postgres:postgres /var/backups/postgres

# Создаем SQL-дамп (версия с параллельным созданием)
sudo -u postgres pg_dump -Fp -d jira -f /var/backups/postgres/jira_db.sql --verbose

# ИЛИ (если нужен custom формат + SQL для надежности)
sudo -u postgres pg_dump -Fc -d jira -f /var/backups/postgres/jira_db.dump
sudo -u postgres pg_dump -Fp -d jira -f /var/backups/postgres/jira_db.sql
```
Перекинуть дамп на машину с Patroni.
```bash
# Копируем оба файла (с jira-cluster на ноут)
scp /var/backups/postgres/jira_db.*   <user_name>@192.168.87.74:/var/backups/postgresql/

# Копируем оба файла (с jira-cluster на pg* master)
scp /var/backups/postgres/jira_db.* root@192.168.45.{201,202,204}:/var/backups/postgresql/

# Копируем оба файла (с ноута на pg* master)
scp /var/backups/postgresql/jira_db_* root@192.168.45.202:/var/backups/postgresql/
```
<br/>

### 3. Восстановить БД на pg* master
#### Проверяем master машину.
```bash
etcdctl --endpoints=http://192.168.45.201:2379,http://192.168.45.202:2379,http://192.168.45.204:2379 endpoint status --write-out=simple | column -t -s ','
patronictl -c /etc/patroni.yml list
```
#### 1. Восстановление на pg2
```bash
# Вариант 1: Восстановление из SQL-дампа (проще для диагностики)

# Удаляем базу данных и пользователя (если существует)
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "DROP ROLE IF EXISTS jira;"

# Создаём пользователя с паролем
sudo -u postgres psql -c "CREATE USER jira WITH PASSWORD 'ваш_пароль';"  # пароль смотрим в /var/atlassian/application-data/jira/dbconfig.xml
# Создаём БД и восстанавливаем
#sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira ENCODING 'UTF8' LC_COLLATE 'C' LC_CTYPE 'C.UTF-8';"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql
# Создаём БД и альтерантивно восстанавливаем
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db_010825.sql > restore.log 2>&1 &
tail -f restore.log  # Мониторинг прогресса

# Вариант 2: Восстановление из custom-дампа (быстрее)
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump
```
[comment]: # (Этот текст не будет отображаться в рендеринге)
[//]: # (Этот текст не будет отображаться в рендеринге)

#### 2. Настройка прав доступа
```bash
sudo -u postgres psql -d jira -c "ALTER USER jira WITH LOGIN;"  # если создали без пароля
sudo -u postgres psql -c "ALTER USER jira WITH PASSWORD 'your_secure_password';"  # пароль смотрим в /var/atlassian/application-data/jira/dbconfig.xml
sudo -u postgres psql -d jira -c "GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO jira;"
sudo -u postgres psql -d jira -c "GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO jira;"
```

#### 3. Проверка восстановления
```bash
# Проверяем таблицы
sudo -u postgres psql -d jira -c "\dt+"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"jiraissue\";"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"

# Проверяем владельца БД
sudo -u postgres psql -c "\l jira"
```

#### Дополнительные рекомендации:

1. **Если возникают ошибки при восстановлении**:
   ```bash
   # Восстанавливаем с игнорированием ошибок (не для production!)
   sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql 2>restore_errors.log
   ```

2. **Для больших баз**:
   ```bash
   # Увеличиваем таймауты
   sudo -u postgres psql -c "ALTER SYSTEM SET statement_timeout = 0;"
   sudo -u postgres psql -c "ALTER SYSTEM SET lock_timeout = 0;"
   sudo systemctl restart patroni
   ```

3. **Проверка целостности**:
   ```bash
   sudo -u postgres psql -d jira -c "VACUUM ANALYZE;"
   sudo -u postgres psql -d jira -c "REINDEX DATABASE jira;"
   ```

4. **Логирование процесса**:
   ```bash
   # Для SQL-дампа
   sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql > restore.log 2>&1

   # Для custom-дампа
   sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump > restore.log 2>&1
   ```
После успешного восстановления не забудьте проверить репликацию на других узлах кластера.


#### 4. Восстановление БД
##### Для SQL-дампа:
```bash
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql
```

##### Для бинарного дампа:
```bash
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump
```

#### 5. Проверка восстановления
```bash
sudo -u postgres psql -d jira -c "\dt+"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"
```

#### 6. Исправляем пользователя jira (на мастере)
```bash
sudo -u postgres psql -c "ALTER USER jira WITH LOGIN;"
sudo -u postgres psql -c "ALTER USER jira WITH PASSWORD 'your_secure_password';"
```

#### 7. Проверяем репликацию
```bash
# На мастере (pg2):
sudo -u postgres psql -c "SELECT * FROM pg_stat_replication;"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"

# На репликах:
sudo -u postgres psql -c "SELECT * FROM pg_stat_wal_receiver;"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"
```

#### 8. Если таблицы не реплицируются
```bash
# На мастере:
sudo -u postgres psql -d jira -c "CREATE PUBLICATION jira_pub FOR ALL TABLES;"
sudo -u postgres psql -d jira -c "SELECT * FROM pg_publication_tables;"

# На репликах:
sudo -u postgres psql -d jira -c "CREATE SUBSCRIPTION jira_sub CONNECTION 'host=pg2 dbname=jira user=replicator' PUBLICATION jira_pub;"
```

#### 9. Финализация
```bash
# Даем права replicator для репликации
sudo -u postgres psql -c "GRANT SELECT ON ALL TABLES IN SCHEMA public TO replicator;"
sudo -u postgres psql -c "ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO replicator;"
```
<br/>

### 4. Проверим инфо о текущей БД в Patroni
Как уже помните, Patroni в виде дублирующией схемы установлен на машин **[pg1](https://192.168.87.6:8006/#v1:0:=lxc%2F201:4::::::11:2)**, **[pg2](https://192.168.87.6:8006/#v1:0:=lxc%2F202:4::::::11:2)**, **[pg3](https://192.168.87.6:8006/#v1:0:=lxc%2F204:4::::::11:2)**.

Проверить конфигурацию командой `patronictl -c /etc/patroni.yml edit-config`, там должны быть примерно такие строки:
```ini
postgresql:
# ***
  pg_hba:
  - host replication replicator 192.168.45.0/24 md5
  - host all all 192.168.45.0/24 md5
  - host all all 192.168.46.0/24 md5
```
<br/>
<br/>



## 4. Установка HAproxy на jira-cluster

Чтобы в файле `/var/atlassian/application-data/jira/dbconfig.xml` не менять IP в строке **`<url>jdbc:postgresql://127.0.0.1:5432/jira</url>`** на IP машины Patroni, можно установить HAproxy на jira-cluster, т.к. в последующем мы склонируем машину jira-cluster.

<details>
<summary>❗haproxy.cfg❗</summary>

```cfg
## /etc/haproxy/haproxy.cfg
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    timeout connect 10s
    timeout client 30s
    timeout server 30s

frontend stats
    bind *:7000
    mode http
    stats enable
    stats uri /
    stats refresh 10s
    stats admin if TRUE

backend postgres_master
    mode tcp
    balance first
    option httpchk GET /master
    http-check expect status 200
    default-server inter 10s fall 3 rise 2 on-marked-down shutdown-sessions
    server pg2 192.168.45.202:5432 check port 8008 inter 10s rise 3 fall 3
    server pg1 192.168.45.201:5432 check port 8008 inter 10s rise 3 fall 3
    server pg3 192.168.45.204:5432 check port 8008 inter 10s rise 3 fall 3

#backend postgres_replicas
#    mode tcp
#    balance roundrobin
#    option httpchk GET /replica
#    http-check expect status 200
#    default-server inter 10s fall 3 rise 2 on-marked-down shutdown-sessions
#    server pg1 192.168.45.201:5432 check port 8008 inter 10s rise 3 fall 3
#    server pg3 192.168.45.204:5432 check port 8008 inter 10s rise 3 fall 3

#frontend postgres_proxy
#    bind *:5000
#    mode tcp
#    default_backend postgres_master
```
</details>

Убедитесь, что Patroni API доступен:
```
root@jira-cluster /var/atlassian/application-data/jira # curl -s http://192.168.45.202:8008/master | jq .state
"running"
```
Мониторинг состояния HAProxy:
```
watch -n 1 'echo "show servers state" | socat /run/haproxy/admin.sock stdio'
```

Доступ к HAProxy через IP: http://192.168.46.2:7000/

Jira Dashboard
https://192.168.46.2/secure/Dashboard.jspa
