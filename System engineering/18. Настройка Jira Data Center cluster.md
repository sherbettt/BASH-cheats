**Читай статью с оф. сайта: [Set up a Jira Data Center cluster](https://confluence.atlassian.com/adminjiraserver/set-up-a-jira-data-center-cluster-993929600.html)**

## 1. Клонирование LXC с Jira

<details>
<summary>❗ ProxMox CLuster ❗</summary>

 Заходим в кластер по одной из ссылок:
- https://192.168.87.17:8006/#v1:0:18:4::::::: - prox4;
- https://192.168.87.20:8006/#v1:0:18:4::::::: - pmx5;
- https://192.168.87.6:8006/#v1:0:18:4::::::: - pmx6.
</details>


**Через GUI.**
Как всегда заходим в [ProxMox](https://192.168.87.6:8006/#v1:0:=lxc%2F169:4::=contentIso:::8:11:), 
ищем контейнер ***Container 169 (jira-new) on node 'prox4'***, в браузере по F12 определяем  IP адрес Jira (192.168.87.219:443), проверяем с помощью nmap. 
Создаём снепшот и после - клонируем машину, присваивая ей новое имя.
```bash
# Сканирование используемых IP адресов
nmap -A 192.168.46.0/24
nmap -sn 192.168.46.0/24
nmap -sn 192.168.46.0/24 -oG - | grep "Up" | awk '{print $2}'
nmap -sn 192.168.46.0/24 | grep "Nmap scan report" | awk '{print $5}'
```

<details>
<summary>❗ Через pvesh, Через pct ❗</summary>
  
**Через pvesh.**
  <br/> Проверяем на всякий случай:
```bash
# зайти
ssh root@192.168.87.17  # prox4
# утилита и команды
pvesh get /nodes/prox4/lxc/169/config
pvesh get /nodes/prox4/lxc/169/interfaces  # 192.168.87.219/24
pvesh get /nodes/prox4/storage  # проверить все места хранения
```

Создаём снепшот:
```bash
# Создаём там же, где и контейнер, на ssd_1tb
# pvesh
pvesh create /nodes/prox4/lxc/169/snapshot \
    --snapname "before-update" \
    --description "Снэпшот перед обновлением Jira"

# pct
pct snapshot 169 "backup-2024" --description "Резервная копия на 2024 год" --vmstate --live
```

Клонируем:
```bash
pvesh create /nodes/prox4/lxc/169/clone \
    --newid 181 \
    --storage ssd_1tb \
    --hostname jira-cluster
```

**Через pct.**
  <br/> Если `pvesh` не работает, можно использовать `pct` (Proxmox Container Toolkit):
```bash
pct clone 169 181 \
    --storage ssd_1tb \
    --hostname jira-cluster
```
</details>


В результате получаем ***Container 181 (jira-cluster) on node 'prox4'*** c IP = 192.168.87.140/24 (желательно выдать static IP). 
<br/> Т.к. сеть 192.168.87.0/24 предназначена в рамках RunTel для выхода в и-нет, то для созданного контейнера важно поменять интерфейс **`eth0`** на **`192.168.46.0/24`** сеть, выбрав свободный IP, в результате выбираем свободный на текущий момент адрес  **`192.168.46.2`** (Bridge = dmznet). 
<br/> Проверяй нашу машину **[102 (dmzgateway)](https://192.168.87.6:8006/#v1:0:=lxc%2F102:4::::::11:2)** на шлюзы.

Чтобы на новой машине ***jira-cluster*** не было конфликта с почтовыми уведомлениями относительно ***jira-new***, необходимо прописать т.н. "ложный путь" временно, до тех пор пока не будет полностью запущен Jira кластер!
```bash
ip route add 77.88.21.125 via 192.168.87.4 dev eth0;
ip route add 77.88.21.125 via 192.168.46.254 dev eth0
```
<br/>
<br/>


## 2. Сопряжение с S3
**ВАЖНО! все машины используют ядро ProxMox - "6.8.12-9-pve", из-за чего работа `nfs-kernel-server` будет поломана**
Проверка конфигурации контейнеров в pvesh и дисков
```
ccat /etc/pve/lxc/181.conf ; ccat /etc/pve/lxc/169.conf
pvesh get /nodes/pmx5/storage
```

Вводные данные:
- **/opt/atlassian/jira/shared** (если используется кластер)
- **/var/atlassian/application-data/jira** (обычно здесь лежат данные)
- **/opt/atlassian/jira/conf/dbconfig.xml**  (конф. файлик для Jira БД PSQL)

### 1. Создать файл `/opt/atlassian/jira/conf/cluster.properties` на машине jira-cluster, jira-node, jira-node2
Требуется создать данный файл на трёх машинах (**192.168.46.2, 192.168.46.3, 192.168.46.4**)
```ini
# jira-cluster
jira.node.id=node1  # node2; node3; node0
jira.shared.home=/var/atlassian/application-data/jira
#jira.shared.home=/opt/atlassian/jira/shared
jira.lb.enabled=true
jira.lb.https=false
jira.lb.hostname=jira.runtel.ru  # или IP
jira.lb.port=8080
```

### 2. Создать/проверить файл `/var/atlassian/application-data/jira/filestore-config.xml` на jira-* маншинах
```ini
<?xml version="1.1" ?>
<filestore-config>
  <filestores>
    <s3-filestore id="attachmentBucket">
      <config> 
        <bucket-name>jira-attachments</bucket-name> 
        <region>ru-runtel1</region>
#        <endpoint-override>http://192.168.87.242:9000</endpoint-override>
        <endpoint-override>https://s3.runtel.org</endpoint-override>
      </config>
    </s3-filestore>
  </filestores>
  <associations>
    <association target="attachments" file-store="attachmentBucket" />
  </associations>
</filestore-config>
```

### 3. Создать файл `/var/atlassian/application-data/jira/hazelcast.properties` на машинах jira-cluster и jira-new
```ini
hazelcast.network.tcpip.members=192.168.87.140,192.168.87.219
hazelcast.group.name=jira-cluster
hazelcast.group.password=securepassword
hazelcast.interface=192.168.87.140 # на первой ноде
# hazelcast.interface=192.168.87.219 # на второй ноде
```

### 4. Логин по адресу.
Зайти на web-интерфейс: **http://192.168.87.140:8080/secure/Dashboard.jspa**
<br/>
<br/>



## 3. Проверка БД PostgreSQL на всех машинах
### 1. Проверить файл конф. БД `/var/atlassian/application-data/jira/dbconfig.xml`
<details>
<summary>❗Jira - dbconfig.xml ❗</summary>

```xml
<?xml version="1.0" encoding="UTF-8"?>

<jira-database-config>
  <name>defaultDS</name>
  <delegator-name>default</delegator-name>
  <database-type>postgres72</database-type>
  <schema-name>public</schema-name>
  <jdbc-datasource>
    <url>jdbc:postgresql://127.0.0.1:5432/jira</url>    # позже изменить на адрес Patroni сервера
    <driver-class>org.postgresql.Driver</driver-class>
    <username>jira</username>
    <password>{ATL_SECURED}</password>
    <pool-min-size>40</pool-min-size>
    <pool-max-size>40</pool-max-size>
    <pool-max-wait>30000</pool-max-wait>
    <validation-query>select 1</validation-query>
    <min-evictable-idle-time-millis>60000</min-evictable-idle-time-millis>
    <time-between-eviction-runs-millis>300000</time-between-eviction-runs-millis>
    <pool-max-idle>40</pool-max-idle>
    <pool-remove-abandoned>true</pool-remove-abandoned>
    <pool-remove-abandoned-timeout>300</pool-remove-abandoned-timeout>
    <pool-test-on-borrow>false</pool-test-on-borrow>
    <pool-test-while-idle>true</pool-test-while-idle>
    <connection-properties>tcpKeepAlive=true</connection-properties>
  </jdbc-datasource>
</jira-database-config>
```
</details>

Потребуется Создать дамп текущей БД и перенести его на машину с Patroni. После переноса и восстановления БД из дампа, ннобходимо будет в строке `<url>jdbc:postgresql://127.0.0.1:5432/jira</url>` указать адрес машины Patroni.
<br/>

### 2. Создать дамп текущей БД и перенести его на машину Patroni
Читай раздел **[Создание дампа БД PostgreSQL](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/16.%20PSQL%20dump%3A%20клон%20%2B%20восстановление.md#-4-создание-дампа-бд-postgresql)**
```bash
# должна быть БД с именем "jira"
sudo -u postgres psql -c "\du" ;
sudo -u postgres psql -c "\l" ;
```
```bash
# Переходим в каталог с бэкапами
cd /var/backups/postgres
chown -R postgres:postgres /var/backups/postgres

# Создаем SQL-дамп (версия с параллельным созданием)
sudo -u postgres pg_dump -Fp -d jira -f /var/backups/postgres/jira_db.sql --verbose

# ИЛИ (если нужен custom формат + SQL для надежности)
sudo -u postgres pg_dump -Fc -d jira -f /var/backups/postgres/jira_db.dump
sudo -u postgres pg_dump -Fp -d jira -f /var/backups/postgres/jira_db.sql
```
Перекинуть дамп на машину с Patroni.
```bash
# Копируем оба файла (с jira-cluster на ноут)
scp /var/backups/postgres/jira_db.*   <user_name>@192.168.87.74:/var/backups/postgresql/

# Копируем оба файла (с jira-cluster на pg* master)
scp /var/backups/postgres/jira_db.* root@192.168.45.{201,202,204}:/var/backups/postgresql/

# Копируем оба файла (с ноута на pg* master)
scp /var/backups/postgresql/jira_db_* root@192.168.45.202:/var/backups/postgresql/
```
<br/>

### 3. Восстановить БД на pg* master
#### Проверяем master машину.
```bash
etcdctl --endpoints=http://192.168.45.201:2379,http://192.168.45.202:2379,http://192.168.45.204:2379 endpoint status --write-out=simple | column -t -s ','
patronictl -c /etc/patroni.yml list
```
#### 1. Восстановление на pg2
```bash
# Вариант 1: Восстановление из SQL-дампа (проще для диагностики)

# Удаляем базу данных и пользователя (если существует)
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "DROP ROLE IF EXISTS jira;"

# Создаём пользователя с паролем
sudo -u postgres psql -c "CREATE USER jira WITH PASSWORD 'ваш_пароль';"  # пароль смотрим в /var/atlassian/application-data/jira/dbconfig.xml
# Создаём БД и восстанавливаем
#sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira ENCODING 'UTF8' LC_COLLATE 'C' LC_CTYPE 'C.UTF-8';"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql
# Создаём БД и альтерантивно восстанавливаем
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db_010825.sql > restore.log 2>&1 &
tail -f restore.log  # Мониторинг прогресса

# Вариант 2: Восстановление из custom-дампа (быстрее)
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump
```
[comment]: # (Этот текст не будет отображаться в рендеринге)
[//]: # (Этот текст не будет отображаться в рендеринге)

#### 2. Настройка прав доступа
```bash
sudo -u postgres psql -d jira -c "ALTER USER jira WITH LOGIN;"  # если создали без пароля
sudo -u postgres psql -c "ALTER USER jira WITH PASSWORD 'your_secure_password';"  # пароль смотрим в /var/atlassian/application-data/jira/dbconfig.xml
sudo -u postgres psql -d jira -c "GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO jira;"
sudo -u postgres psql -d jira -c "GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO jira;"
```

#### 3. Проверка восстановления
```bash
# Проверяем таблицы
sudo -u postgres psql -d jira -c "\dt+"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"jiraissue\";"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"

# Проверяем владельца БД
sudo -u postgres psql -c "\l jira"
```

#### Дополнительные рекомендации:

1. **Если возникают ошибки при восстановлении**:
   ```bash
   # Восстанавливаем с игнорированием ошибок (не для production!)
   sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql 2>restore_errors.log
   ```

2. **Для больших баз**:
   ```bash
   # Увеличиваем таймауты
   sudo -u postgres psql -c "ALTER SYSTEM SET statement_timeout = 0;"
   sudo -u postgres psql -c "ALTER SYSTEM SET lock_timeout = 0;"
   sudo systemctl restart patroni
   ```

3. **Проверка целостности**:
   ```bash
   sudo -u postgres psql -d jira -c "VACUUM ANALYZE;"
   sudo -u postgres psql -d jira -c "REINDEX DATABASE jira;"
   ```

4. **Логирование процесса**:
   ```bash
   # Для SQL-дампа
   sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql > restore.log 2>&1

   # Для custom-дампа
   sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump > restore.log 2>&1
   ```
После успешного восстановления не забудьте проверить репликацию на других узлах кластера.


#### 4. Восстановление БД
##### Для SQL-дампа:
```bash
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres psql -d jira -f /var/backups/postgresql/jira_db.sql
```

##### Для бинарного дампа:
```bash
sudo -u postgres psql -c "DROP DATABASE IF EXISTS jira;"
sudo -u postgres psql -c "CREATE DATABASE jira WITH OWNER jira;"
sudo -u postgres pg_restore -Fc -d jira -j 4 -v /var/backups/postgresql/jira_db.dump
```

#### 5. Проверка восстановления
```bash
sudo -u postgres psql -d jira -c "\dt+"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"
```

#### 6. Исправляем пользователя jira (на мастере)
```bash
sudo -u postgres psql -c "ALTER USER jira WITH LOGIN;"
sudo -u postgres psql -c "ALTER USER jira WITH PASSWORD 'your_secure_password';"
```

#### 7. Проверяем репликацию
```bash
# На мастере (pg2):
sudo -u postgres psql -c "SELECT * FROM pg_stat_replication;"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"

# На репликах:
sudo -u postgres psql -c "SELECT * FROM pg_stat_wal_receiver;"
sudo -u postgres psql -d jira -c "SELECT count(*) FROM \"AO_0201F0_KB_HELPFUL_AGGR\";"
```

#### 8. Если таблицы не реплицируются
```bash
# На мастере:
sudo -u postgres psql -d jira -c "CREATE PUBLICATION jira_pub FOR ALL TABLES;"
sudo -u postgres psql -d jira -c "SELECT * FROM pg_publication_tables;"

# На репликах:
sudo -u postgres psql -d jira -c "CREATE SUBSCRIPTION jira_sub CONNECTION 'host=pg2 dbname=jira user=replicator' PUBLICATION jira_pub;"
```

#### 9. Финализация
```bash
# Даем права replicator для репликации
sudo -u postgres psql -c "GRANT SELECT ON ALL TABLES IN SCHEMA public TO replicator;"
sudo -u postgres psql -c "ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO replicator;"
```
<br/>

### 4. Проверим инфо о текущей БД в Patroni
Как уже помните, Patroni в виде дублирующией схемы установлен на машин **[pg1](https://192.168.87.6:8006/#v1:0:=lxc%2F201:4::::::11:2)**, **[pg2](https://192.168.87.6:8006/#v1:0:=lxc%2F202:4::::::11:2)**, **[pg3](https://192.168.87.6:8006/#v1:0:=lxc%2F204:4::::::11:2)**.

Проверить конфигурацию командой `patronictl -c /etc/patroni.yml edit-config`, там должны быть примерно такие строки:
```ini
postgresql:
# ***
  pg_hba:
  - host replication replicator 192.168.45.0/24 md5
  - host all all 192.168.45.0/24 md5
  - host all all 192.168.46.0/24 md5
```
<br/>
<br/>



## 4. Установка HAproxy на jira-cluster

Чтобы в файле `/var/atlassian/application-data/jira/dbconfig.xml` не менять IP в строке **`<url>jdbc:postgresql://127.0.0.1:5432/jira</url>`** на IP машины Patroni, можно установить HAproxy на jira-cluster, т.к. в последующем мы склонируем машину jira-cluster.

<details>
<summary>❗haproxy.cfg❗</summary>

```cfg
## /etc/haproxy/haproxy.cfg
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    timeout connect 10s
    timeout client 30s
    timeout server 30s

frontend stats
    bind *:7000
    mode http
    stats enable
    stats uri /
    stats refresh 10s
    stats admin if TRUE

backend postgres_master
    mode tcp
    balance first
    option httpchk GET /master
    http-check expect status 200
    default-server inter 10s fall 3 rise 2 on-marked-down shutdown-sessions
    server pg2 192.168.45.202:5432 check port 8008 inter 10s rise 3 fall 3
    server pg1 192.168.45.201:5432 check port 8008 inter 10s rise 3 fall 3
    server pg3 192.168.45.204:5432 check port 8008 inter 10s rise 3 fall 3

#backend postgres_replicas
#    mode tcp
#    balance roundrobin
#    option httpchk GET /replica
#    http-check expect status 200
#    default-server inter 10s fall 3 rise 2 on-marked-down shutdown-sessions
#    server pg1 192.168.45.201:5432 check port 8008 inter 10s rise 3 fall 3
#    server pg3 192.168.45.204:5432 check port 8008 inter 10s rise 3 fall 3

frontend postgres_proxy
    bind *:5000
    mode tcp
    default_backend postgres_master
```
</details>

Убедитесь, что Patroni API доступен:
```
root@jira-cluster /var/atlassian/application-data/jira # curl -s http://192.168.45.202:8008/master | jq .state
"running"
```
Мониторинг состояния HAProxy:
```
watch -n 1 'echo "show servers state" | socat /run/haproxy/admin.sock stdio'
```

Доступ к HAProxy через IP: http://192.168.46.2:7000/

Jira Dashboard
https://192.168.46.2/secure/Dashboard.jspa


## 5. Переместить машины jira-* на разные ноды ProxMox
Если вы вдруг стоклнётесь с ошибкой примерно следующего рода:
```
2025-08-04 14:32:22 93529165824 bytes (94 GB, 87 GiB) copied, 8040 s, 11.6 MB/s
2025-08-04 14:32:52 93878636544 bytes (94 GB, 87 GiB) copied, 8070 s, 11.6 MB/s
2025-08-04 14:33:22 94227845120 bytes (94 GB, 88 GiB) copied, 8100 s, 11.6 MB/s
2025-08-04 14:33:48 543+3164245 records in
2025-08-04 14:33:48 543+3164244 records out
2025-08-04 14:33:48 94541963264 bytes (95 GB, 88 GiB) copied, 8127.16 s, 11.6 MB/s
2025-08-04 14:33:52 command 'dd 'of=/stg/1tb/images/184/vm-184-disk-0.raw' 'conv=sparse' 'bs=64k'' failed: exit code 1
2025-08-04 14:33:52 command 'dd 'if=/stg/1tb/images/184/vm-184-disk-0.raw' 'bs=4k' 'status=progress'' failed: got signal 13
2025-08-04 14:33:52 ERROR: storage migration for 'ssd_1tb:184/vm-184-disk-0.raw' to storage 'ssd_1tb' failed - command 'set -o pipefail && pvesm export ssd_1tb:184/vm-184-disk-0.raw raw+size - -with-snapshots 0 | /usr/bin/ssh -e none -o 'BatchMode=yes' -o 'HostKeyAlias=pmx5' -o 'UserKnownHostsFile=/etc/pve/nodes/pmx5/ssh_known_hosts' -o 'GlobalKnownHostsFile=none' root@192.168.87.20 -- pvesm import ssd_1tb:184/vm-184-disk-0.raw raw+size - -with-snapshots 0 -allow-rename 1' failed: exit code 1
2025-08-04 14:33:52 aborting phase 1 - cleanup resources
2025-08-04 14:33:52 ERROR: found stale volume copy 'ssd_1tb:184/vm-184-disk-0.raw' on node 'pmx5'
2025-08-04 14:33:52 start final cleanup
2025-08-04 14:33:52 ERROR: migration aborted (duration 02:15:32): storage migration for 'ssd_1tb:184/vm-184-disk-0.raw' to storage 'ssd_1tb' failed - command 'set -o pipefail && pvesm export ssd_1tb:184/vm-184-disk-0.raw raw+size - -with-snapshots 0 | /usr/bin/ssh -e none -o 'BatchMode=yes' -o 'HostKeyAlias=pmx5' -o 'UserKnownHostsFile=/etc/pve/nodes/pmx5/ssh_known_hosts' -o 'GlobalKnownHostsFile=none' root@192.168.87.20 -- pvesm import ssd_1tb:184/vm-184-disk-0.raw raw+size - -with-snapshots 0 -allow-rename 1' failed: exit code 1
TASK ERROR: migration aborted
```
то рекомендуется произвести ручные действия из cli:

 1. Проверить скорость сети и увеличить, если она небольшая
    ```bash
    ethtool enp4s0
    ethtool -s enp4s0 speed 1000 duplex full autoneg on
    ```
 2. Проверка состояния хранилища:
    ```bash
    pvesm status
    pvesm list ssd_1tb
    pvesh get /nodes/pmx5/storage
    ```
 3. Проверить наличие контейнера:
    ```bash
    qm list | grep 184  # на pmx5 и на prox4
    ```
 4. Проверка состояния исходного тома:
    ```bash
    pvesm path ssd_1tb:184/vm-184-disk-0.raw 
    ls -alhF /stg/1tb/images/184/vm-184-disk-0.raw
    ```
 5. Очистка оставшихся артефактов и бэкапов (после ошибки):
    ```bash
    pvesm free ssd_1tb:184/vm-184-disk-0.raw --storage ssd_1tb --vmid 184
    pvesm free ssd_1tb:backup/vzdump-lxc-201-2025_07_21-13_27_45.tar.zst
    rm -f /var/lib/vz/dump/*.tar.zst  # ОЧЕНЬ ОСТОРОЖНО!
    ```
 6. Проверить конф. файл контейнера:
    ```bash
    ll /etc/pve/lxc/  # 184.conf  в нашем случае на prox4
    ```
 7. Скопировать конф. файл контейнера c prox4 на pmx5:
    ```bash
    scp /etc/pve/lxc/184.conf  root@192.168.87.20:/etc/pve/lxc/
    # или луше
    mv /etc/pve/lxc/184.conf 
    ```
 8. RSYNC скопировать диск:
    ```bash
    rsync -avzP /stg/1tb/images/184/vm-184-disk-0.raw root@192.168.87.20:/stg/1tb/images/184/
    rsync -avP /stg/1tb/images/184 root@192.168.87.20:/stg/1tb/images/
    rsync -avP --sparse /stg/1tb/images/184 root@192.168.87.20:/stg/1tb/images/
    ```
 9. Альтернативно копировать с помощью dd:
    ```bash
    dd if=/stg/1tb/images/184/vm-184-disk-0.raw bs=4M status=progress | gzip | ssh root@192.168.87.20 "gunzip > /stg/1tb/images/184/vm-184-disk-0.raw"
    ```
 10. Проверить журналы на обоих серверах:
    ```bash
    journalctl -u pveproxy -n 50
    journalctl -u pvestatd -n 50
    ```
<br/>


## 6. NGINX: конфигурирование (192.168.87.2)
На LXC контейнер dmzgateway установлен NGINX, который является обычным Proxy.

<details>
<summary>❗ jira.runtel.ru ❗</summary>

```c
### /etc/nginx/sites-enabled/jira.runtel.ru
### 192.168.87.2

upstream jira_servers {
#    keepalive 32;
    ip_hash;
    server 192.168.87.219:8080;
#    server 192.168.46.2:8080 backup;
#    server 192.168.46.3:8080;
#    server 192.168.46.4:8080 backup;
}

upstream jira_admin {
    server 192.168.87.219:8080;
#    server 192.168.46.2:8080;
#    server 192.168.46.3:8080 backup;
#    server 192.168.46.4:8080 backup;
}

# There are  any-other-domain.com:8443  requests
# Call-all server
server {
    listen 443 ssl default_server;
    listen [::]:443 ssl default_server;
    server_name _;
    
    # Really sertificates haven't beed used
    ssl_certificate /etc/nginx/runtel.pem;
    ssl_certificate_key /etc/nginx/runtel.pem;
    
    ssl_protocols TLSv1.2 TLSv1.3;
    return 444;       # Close session without answer
}


# There are jira.runtel.ru  requests
server {
    listen 443 ssl;  # 
    listen [::]:443 ssl;
    server_name jira.runtel.ru;

    
    # SSL conf
    ssl_certificate /etc/nginx/runtel.pem;
    ssl_certificate_key /etc/nginx/runtel.pem;
#    ssl_protocols  TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;
#    ssl_ciphers         HIGH:!aNULL:!MD5;
#    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384';
#    ssl_prefer_server_ciphers on;
#    ssl_session_cache shared:SSL:10m;
#    ssl_session_timeout 10m;
#    ssl_session_tickets off;
    
    # Security headers
#    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";
#    add_header X-Frame-Options DENY;
#    add_header X-Content-Type-Options nosniff;
#    add_header Referrer-Policy strict-origin-when-cross-origin;
    
    # Proxy settings
#    proxy_ssl_server_name on;
    proxy_ssl_verify off;
    
    location / {
		client_max_body_size 20m;
	        proxy_pass http://jira_servers;
	        proxy_set_header Upgrade $http_upgrade;
	        proxy_set_header Connection 'Upgrade';
	        proxy_read_timeout 1200;
	        proxy_connect_timeout 1200;
#	        proxy_next_upstream error timeout invalid_header http_500 http_502 http_403;	    
		proxy_set_header X-Forwarded-Proto $scheme;
	        proxy_set_header Host $host;
	        proxy_set_header X-Real-IP $remote_addr;
	        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
#    location /admin/ {
#                client_max_body_size 20m;
#                proxy_pass http://jira_admin;
#                proxy_set_header Upgrade $http_upgrade;
#                proxy_set_header Connection 'Upgrade';
#                proxy_read_timeout 1200;
#                proxy_connect_timeout 1200;
#                proxy_next_upstream error timeout invalid_header http_500;
#                proxy_set_header X-Forwarded-Proto $scheme;
#                proxy_set_header Host $host;
#                proxy_set_header X-Real-IP $remote_addr;
#                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
#
#    }
    
#    location = /health {
#        access_log off;
#        proxy_pass https://kc_servers;
#    }
    
#    error_page 502 503 504 /maintenance.html;
#    location = /maintenance.html {
#        root /var/www/html;
#        internal;
#    }
}
```
</details>

За основу использован пример из статьи **[System engineering/17. keycloak: резервный сервер.md](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/17.%20keycloak%3A%20резервный%20сервер.md)** -> объяснение в [NGINX conf. file: sso.runtel.ru .md](https://github.com/sherbettt/BASH-cheats/blob/main/System%20engineering/NGINX%20conf.%20file%3A%20sso.runtel.ru%20.md)
<br/>


## 7. Плавающий IP: keepalived
Такой файл нужно раскидать на все "железки" ProxMox кластера. 
Условно IP=192.168.87.3/24 принят за плавающий
```c
root@prox4 ~ # ccat /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
        state BACKUP
        interface vmbr0
        virtual_router_id 51
        priority 255
        advert_int 1
        authentication {
              auth_type PASS
              auth_pass RFyMUDEuya6wAG2V
        }
        virtual_ipaddress {
              192.168.87.3/24
        }
}
```

В файле **`/etc/fstab`** на **prox4,pmx5 и pmx6** добавить строку внизу:
```
192.168.87.3:/stg/8tb/share/    /mnt/share/    nfs    rw,bg,soft,intr,nosuid    0 0
```
Эта строка добавляет автоматическое монтирование NFS-шары при загрузке системы.
1. **192.168.87.3:/stg/8tb/share/** - источник (NFS-сервер и экспортируемый путь)
2. **/mnt/share/** - точка монтирования (локальная директория)
3. **nfs** - тип файловой системы (Network File System)
4. **rw,bg,soft,intr,nosuid** - опции монтирования:
   - `rw`: read-write (чтение и запись)
   - `bg`: background (монтировать в фоновом режиме при ошибках)
   - `soft`: soft mount (прекращать попытки при ошибках после таймаута)
   - `intr`: interruptible (разрешить прерывание операций)
   - `nosuid`: игнорировать suid/sgid биты (мера безопасности)
5. **0 0** - опции для dump и fsck (не используются для NFS)

После добавления строки нужно либо перезагрузить сервер, либо выполнить `mount -a` для применения изменений.
