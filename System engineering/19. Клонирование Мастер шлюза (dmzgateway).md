# Создание клонов/кластера машины 102 (dmzgateway) для обеспечения устойчивости сети.


## 1. Установить утилиту keepalived на машину 102 (dmzgateway)
Обновить систему на всякий случай:
```bash
sudo apt update && sudo apt upgrade -y
```

Установка keepalived
```bash
sudo apt install keepalived -y
```

Проверка статуса
```bash
sudo systemctl status keepalived.service -l --no-pager
```

Запуск и включение в автозагрузку сделаем чуть позже, т.к. в нашем случае потребуется настроить сперва конфигурационный файл `/etc/keepalived/keepalived.conf`, а после сделать копии с оригинальной машины.
```bash
sudo systemctl enable --now keepalived
```
<br/>


## 2. Настройка конфигурационного файла
К сведению: IP адрес = `192.168.87.3` - является **плавающим** для "железных" машин кластера.
Образец можно посмотреть на "железных" машинах [pmx5](https://192.168.87.20:8006/#v1:0:=node%2Fpmx5:4:11::::::), [pmx6](https://192.168.87.17:8006/#v1:0:=node%2Fpmx6:4:11:::::11:), [prox4](https://192.168.87.17:8006/#v1:0:=node%2Fprox4:4:11::::::). 

Планриуется настройка Keepalived для трёх нод (dmzgateway1, dmzgateway2, dmzgateway3).
Схема должна выглядет следующим образом:

Адреса оригинальной машины станут плавающими:
- *`192.168.87.2`*  **eth0** → **(vmbr0):Bridge** 
- *`192.168.46.1`*  **eth1** → **(dmznet):Bridge** 
- *`192.168.45.1`*  **eth2** → **(pgnet):Bridge** 

Физические адреса нод:
| Нода            | vmbr0              | dmznet             | pgnet           |  
|-----------------|--------------------|--------------------|--------------------|  
| **dmzgateway1** | **192.168.87.253** | **192.168.46.253** | **192.168.45.253** |  
| **dmzgateway2** | **192.168.87.252** | **192.168.46.252** | **192.168.45.252** |  
| **dmzgateway3** | **192.168.87.251** | **192.168.46.251** | **192.168.45.251** |  
| **Gateway**     | **192.168.87.1**   | N/A                | N/A             |  


Также читай статью: [Пример использования keepalived на Linux](https://www.dmosk.ru/miniinstruktions.php?mini=keepalived-linux#settings)

### 1. Проверям интерфейсы
```bash
ip -br -c addr show; 
ip -c addr show;
```

### 2. Создаём конф. файл

<details>
<summary>❗keepalived.conf❗</summary>
  
```c
global_defs {
    router_id dmzgateway1
}

vrrp_instance VI_VMBR0 {
    state BACKUP           # Все ноды стартуют как BACKUP
    interface eth0         # Интерфейс для VRRP
    virtual_router_id 51   # Должен быть одинаковым в кластере
    priority 100           # Одинаковый приоритет на всех нодах
    advert_int 1           # Частота объявлений (сек)
    nopreempt              # Запрещает перехват роли мастера
    virtual_ipaddress {
        192.168.87.2/24    # Виртуальный IP
    }
}

vrrp_instance VI_DMZNET {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.46.1/24
    }
}

vrrp_instance VI_PGNET {
    state BACKUP
    interface eth2
    virtual_router_id 53
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.45.1/24
    }
}
```
</details>  

```bash
##/etc/keepalived/{master,backup}.sh
#!/bin/bash
echo "[$(date)] Нода $HOSTNAME перешла в состояние $1" >> /var/log/keepalived-state.log
```

`state BACKUP` и `priority 100` одинаковый на всех машинах, т.к. приоритета нет к машинам после перезагрузки

И проверяем синтаксис:
```bash
keepalived -t
```
Проверяем маршруты
```bash
ip route show
```
Проверка на скрытыие символы в пароле:
```bash
hexdump -C /etc/keepalived/keepalived.conf | grep -A5 "auth_pass"
```
Проверка VIP на одной из нод:
```bash
ip addr show eth0 | grep 192.168.87.2
```
Проверка логов:
```bash
journalctl -u keepalived -f --no-pager
```

Отредактировать данный файл на остальных нодах после клонирования оригинальной машины.
<br/>



## 3. Клонируем машины
Используя GUI, может возникнуть ошибка при клонировании контейнера в ProxMox:
***unable to clone mountpoint 'mp0' (type bind) (500)***

Эта ошибка возникает при попытке клонирования контейнера LXC в Proxmox VE, когда у исходного контейнера есть привязанные (bind) точки монтирования. 

В рамках GUI можно сделать Deattach Mount Point, т.к. в нашем случае он не нужен.

**Причины проблемы:**
- Контейнер имеет bind-монтирования (тип mp0)
- Эти монтирования ссылаются на пути на хосте, которые не существуют или недоступны
- Proxmox не может автоматически обработать такие монтирования при клонировании

**Решение:**
1) Зайти на "железный" сервер, где хранится контейнер и проверить конфигурацию lxc контейнера.
```bash
root@pmx6 ~ > ccat /etc/pve/lxc/102.conf 
arch: amd64
cores: 1
features: fuse=1,mknod=1,nesting=1
hostname: dmzgateway
memory: 1024
mp0: /stg/8tb/share,mp=/shared,backup=0
net0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.87.1,hwaddr=BC:24:11:AD:EC:E2,ip=192.168.87.2/24,type=veth
net1: name=eth1,bridge=dmznet,hwaddr=BC:24:11:E6:5D:BD,ip=192.168.46.1/24,type=veth
net2: name=eth2,bridge=pgnet,firewall=1,hwaddr=BC:24:11:C4:6E:72,ip=192.168.45.1/24,type=veth
onboot: 1
ostype: debian
rootfs: ssd_1tb:102/vm-102-disk-0.raw,size=10G
swap: 0
unprivileged: 1
```
Строку временно до клонирования поменять на:
```bash
mp0: /stg/8tb/share,mp=/shared,backup=0,mountoptions=noatime
```

2) Склонировать с помощью pct.
```bash
vzdump 102
pct restore NEWID /var/lib/vz/dump/vzdump-lxc-102-*.tar.gz
```
<br/>



## 4. Установка и настройка csync2 через Ad-Hoc Ansible команды
Создаём ansible.cfg файлик:
```cfg
[defaults]
inventory = ./inventory/hosts.ini        # либо ~/.ansible/infra-proj/inventory/hosts.ini
private_key_file = ~/.ssh/id_rsa         # Путь к вашему приватному ключу
host_key_checking = False                # Отключает проверку SSH-ключей хостов
interpreter_python = /usr/bin/python3    # какой интерпретатор Python использовать на управляемых узлах.
```
### 1. Проверка подключения.
```bash
└─ ~/.ansible/infra-proj $ ansible gateways -m ping
dmzgateway2 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway1 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway3 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
  #-------

ansible gateways -m shell -a "nc -zv dmzgateway1 30865"
ansible gateways -m shell -a "nc -zv dmzgateway2 30865"
ansible gateways -m shell -a "nc -zv dmzgateway3 30865"
```

### 2. Добавим записи в /etc/hosts на всех узлах и проверим
```bash
ansible gateways -m blockinfile -a "path=/etc/hosts block='192.168.87.253 dmzgateway1
192.168.87.252 dmzgateway2
192.168.87.251 dmzgateway3'"
```
```bash
ansible gateways -m shell -a "ping -c 2 dmzgateway1"
ansible gateways -m shell -a "ping -c 2 dmzgateway2"
ansible gateways -m shell -a "ping -c 2 dmzgateway3"
```

### 2. Установка пакета csync2 на все шлюзы
```bash
ansible gateways -m apt -a "name=csync2 state=present update_cache=yes"
# или:
ansible gateways -i hosts -u root -k -m apt -a "name=csync2 state=present update_cache=yes"
```

### 3. Создание конфигурационных каталогов
```bash
ansible gateways -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -m file -a "path=/etc/csync2/key.d state=directory mode=0750"

# или:
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2/key.d state=directory mode=0750"
```

### 4.1. Генерация ключа на dmzgateway1 и копирование на остальные узлы
```bash
# Генерация ключа на dmzgateway1
ansible dmzgateway1 -i inventory/hosts.ini  -m shell -a "csync2 -k /tmp/csync2.key"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m shell -a "csync2 -k /tmp/csync2.key"

# Копирование с dmzgateway1 на локальную машину
ansible dmzgateway1 -i inventory/hosts.ini -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"

# Распространение ключа на все узлы
ansible gateways -i inventory/hosts.ini -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
```

### 4.2. Генерация ключа локальной машине (192.168.87.74) и копирование на остальные узлы
```bash
# Генерация ключа на локальной машине
csync2 -k ./csync2.key

# если Ximper Linux, то будет
sudo find /usr -name csync2
whereis csync2
/usr/sbin/csync2 -k ./csync2.key
chmod 600 ./csync2.key

# Копирование ключа на все узлы
ansible gateways -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
#---------------------------------------------------
```


### 4. Создание конфигурационного файла `/etc/csync2/csync2.cfg`

```bash
ansible gateways -m copy -a "content='group cluster {
    host dmzgateway1(192.168.87.251);
    host dmzgateway2(192.168.87.252);
    host dmzgateway3(192.168.87.253);
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec /usr/sbin/service keepalived restart;
        logfile /var/log/csync2_action.log;
    }
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
Или `/etc/csync2/csync2.cfg` без SSL сертификатов
```conf
nossl * *;
group cluster {
    host dmzgateway1;
    host dmzgateway2;
    host dmzgateway3;
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec /usr/sbin/service keepalived restart;
        logfile /var/log/csync2_action.log;
    }
}
```
Создание симлинка на нужный конф. файл:
```bash
ansible gateways -m shell -a "ln -sf /etc/csync2/csync2.cfg /etc/csync2.cfg"
```
Или перемещение конфига:
```bash
ansible gateways -m shell -a "mv /etc/csync2/csync2.cfg /etc/csync2.cfg"
```


### 5. Создание `/etc/systemd/system/csync2.service` юнита
```bash
ansible gateways -m copy -a "content='[Unit]\nDescription=csync2 cluster synchronization\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -v\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\n' dest=/etc/systemd/system/csync2.service mode=0644" -b
```
т.е.
```ini
[Unit]
Description=csync2 cluster synchronization
After=network.target

[Service]
Type=simple
ExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -ii -v
Restart=on-failure
RestartSec=5s
#
#Environment="CSYNC2_SYSTEM_DIR=/etc/csync2"
#ExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -v

[Install]
WantedBy=multi-user.target
```
Возможно придётся руками задать переменную CSYNC2_SYSTEM_DIR и прописать в `/etc/systemd/system/csync2.service` юнит
```
ansible gateways -m shell -a "ls -l /etc/csync2/csync2.cfg"
ansible gateways -m shell -a "echo CSYNC2_SYSTEM_DIR=$CSYNC2_SYSTEM_DIR"
```





### 6. Включение и запуск службы
```bash
ansible gateways -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -m shell -a "systemctl enable csync2 --now"
ansible gateways -m shell -a "systemctl status csync2"
```

### 7. Инициализация синхронизации
```bash
ansible dmzgateway1 -m shell -a "csync2 -xv"
ansible dmzgateway1 -m shell -a "csync2 -N 192.168.87.251 -xvv"
# или:
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 8. Тест синхронизации
```bash
# Создаем тестовый файл
ansible dmzgateway1 -m copy -a "content='test sync' dest=/etc/keepalived/test_sync.txt"

# Запускаем синхронизацию
ansible dmzgateway1 -m shell -a "csync2 -x"

# Проверяем на всех узлах
ansible gateways -m shell -a "ls -la /etc/keepalived/ | grep test_sync"
```

### 9. Проверка статуса
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ls -la /etc/csync2"
# или:
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```

### 10. Дополнительные проверки
```bash
# Проверка логов
ansible gateways -m shell -a "tail -n 20 /var/log/csync2*"

# Проверка подключений
ansible gateways -m shell -a "netstat -tulpn | grep csync2"
```

### 11. Создание SSL-сертификатов

#### Можно руками на мастер ноде руками создать и руками расзложить по нодам:
```bash
openssl req -x509 -newkey rsa:4096 -keyout csync2_ssl_key.pem -out csync2_ssl_cert.pem -days 3650 -nodes -subj "/CN=dmzgateway1";
# раскидываем:
for node in dmzgateway2 dmzgateway3; do
  scp /etc/csync2_ssl_{key,cert}.pem root@$node:/etc/
done
```
#### Либо попробуем развернуть сертификаты на все узлы через Ansible:
```bash
# Копируем ключ и сертификат
ansible gateways -m copy -a "src=csync2_ssl_key.pem dest=/etc/csync2_ssl_key.pem mode=0600"
ansible gateways -m copy -a "src=csync2_ssl_cert.pem dest=/etc/csync2_ssl_cert.pem mode=0644"

# Альтернативно, можно создать сертификаты прямо на узлах (если есть openssl):
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2/csync2_ssl_key.pem -out /etc/csync2/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
ansible gateways -m shell -a "chmod 600 /etc/csync2/csync2_ssl_key.pem; chmod 644 /etc/csync2/csync2_ssl_cert.pem"
```

#### Если ключи были уже, рекомендуется их удалить предварительно:
```bash
ansible gateways -m shell -a "rm -f /etc/csync2/key.d/csync2.key && csync2 -k /etc/csync2/key.d/csync2.key && chmod 600 /etc/csync2/key.d/csync2.key"
```
Исправить правила iptables:
```bash
ansible gateways -m shell -a "rm -f /etc/iptables && mkdir -p /etc/iptables"
ansible gateways -m shell -a "iptables -A INPUT -p tcp --dport 30865 -j ACCEPT && iptables-save > /etc/iptables/rules.v4"
```
#### Настроить правила iptables в директории `/etc/iptables/`
```bash
# Сначала сохраним текущие правила из /etc/iptables
ansible gateways -m shell -a "cp /etc/iptables /etc/iptables.rules.backup"

# Удалим файл /etc/iptables и создадим вместо него директорию
ansible gateways -m shell -a "rm -f /etc/iptables && mkdir -p /etc/iptables"

# Добавим правило для порта 30865 и сохраним всё в /etc/iptables/rules.v4
ansible gateways -m shell -a "iptables -A INPUT -p tcp --dport 30865 -j ACCEPT && iptables-save > /etc/iptables/rules.v4"

# Восстановим NAT-правила из бэкапа
ansible gateways -m shell -a "cat /etc/iptables.rules.backup >> /etc/iptables/rules.v4"

# Проверим, что всё сохранилось правильно
ansible gateways -m shell -a "cat /etc/iptables/rules.v4"

# Перезагрузим iptables (если нужно)
ansible gateways -m shell -a "iptables-restore < /etc/iptables/rules.v4"

# Проверим, что порт 30865 открыт
ansible gateways -m shell -a "iptables -L -n | grep 30865 || echo 'Порт не открыт'"
```

#### Ad-Hoc командой создаём сертификаты на всех узлах:
```bash
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2_ssl_key.pem -out /etc/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
```
Права:
```bash
ansible gateways -m shell -a "chmod 600 /etc/csync2_ssl_key.pem; chmod 644 /etc/csync2_ssl_cert.pem"
```


Обновляем конфигурацию **csync2**:
```conf
ansible gateways -m copy -a "content='group cluster {
    host (dmzgateway1 dmzgateway2 dmzgateway3);
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    ssl_cert /etc/csync2/csync2_ssl_cert.pem;
    ssl_key /etc/csync2/csync2_ssl_key.pem;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
```bash
ansible gateways -m shell -a "systemctl restart csync2"
ansible gateways -m shell -a "csync2 -xv"
```



```bash
ansible gateways -m shell -a "grep host /etc/csync2/csync2.cfg"
```

### 10. Настройка автоматической синхронизации (опционально)
```bash
# Синхронизация Каждые 5 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/5' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 10 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/10' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 30 минут
ansible gateways -m cron -a "name='30-min csync2 sync' minute='*/30' job='/usr/sbin/csync2 -x'"

# Синхронизация каждый час (в 0 минут каждого часа)
ansible gateways -m cron -a "name='Hourly csync2 sync' minute='0' job='/usr/sbin/csync2 -x'"

# Синхронизация каждые 4 часа (в 0 минут каждые 4 часа)
ansible gateways -m cron -a "name='4-hour csync2 sync' minute='0' hour='*/4' job='/usr/sbin/csync2 -x'"

## Удаление cron-заданий (если нужно)
# Удалить ежечасную синхронизацию
ansible gateways -m cron -a "name='Hourly csync2 sync' state=absent"

# Удалить синхронизацию каждые 4 часа
ansible gateways -m cron -a "name='4-hour csync2 sync' state=absent"
```
Проверка cron триггеров:
```
ansible gateways -m shell -a "crontab -l | grep csync2"
```


### Ключевые особенности:
1. Все команды используют SSH-ключ (как настроено в ansible.cfg)
2. Ключ генерируется один раз локально
3. Конфигурация применяется одинаково на всех узлах
4. Не требуется ввод паролей
5. Для аутентификации используется пароль root (`-k` флаг)
6. Ключ генерируется один раз на dmzgateway1 и распространяется на остальные узлы

### Дополнительные проверки:
```bash
# Проверка синхронизированных файлов
ansible gateways -i hosts -u root -k -m shell -a "ls -la /etc/csync2"

# Проверка логов
ansible gateways -i hosts -u root -k -m shell -a "tail -n 20 /var/log/csync2*"
```

--------------------------------------------------------
### Если xinetd уже занимает порт 30865:
```bash
# Проверка порта
ansible gateways -m shell -a "netstat -tulpn | grep 30865 || ss -tulpn | grep 30865"

# Остановим xinetd (так как csync2 должен работать самостоятельно)
ansible gateways -m shell -a "systemctl stop xinetd && systemctl disable xinetd"

# Убедимся, что порт освободился
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```
```bash
# Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"

# Проверим статус
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2 || echo 'csync2 not listening'"

# Инициализируем синхронизацию
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xvv"
```

#### Вариант A: Настроим csync2 работать через xinetd
```bash
# 1. Включим xinetd обратно
ansible gateways -m shell -a "systemctl enable xinetd --now"

# 2. Настроим конфиг xinetd для csync2
ansible gateways -m copy -a "content='service csync2
{
    disable = no
    socket_type = stream
    protocol = tcp
    wait = no
    user = root
    server = /usr/sbin/csync2
    server_args = -i
    only_from = 192.168.87.0/24
}' dest=/etc/xinetd.d/csync2 mode=0640"

# 3. Перезапустим xinetd
ansible gateways -m shell -a "systemctl restart xinetd"
```

#### Вариант B: Изменим порт csync2 (если нужно оставить xinetd для других сервисов)
```bash
# В конфиге /etc/csync2/csync2.cfg добавьте:
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# Затем перезапустите:
ansible gateways -m shell -a "systemctl restart csync2"
```

#### Важно:
1. Выберите только один вариант (A или B)
2. После настройки проверьте:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (использовать xinetd), так как это стандартный подход в ALT Linux для csync2.

--------------------------------------------------------
### Если openbsd-inetd (простой inetd) уже занимает порт 30865:
### 1. Остановим inetd и освободим порт 30865:
```bash
ansible gateways -m shell -a "systemctl stop openbsd-inetd && systemctl disable openbsd-inetd"
```

### 2. Проверим освобождение порта:
```bash
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```

### 3. Перезапустим csync2:
```bash
ansible gateways -m shell -a "systemctl restart csync2"
```

### 4. Проверим работу csync2:
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2"
```

### 5. Инициализируем синхронизацию:
```bash
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xv"  # как бы на одной ноде делается
ansible gateways -m shell -a "/usr/sbin/csync2 -xv"
```

### Если нужно оставить inetd для других сервисов:

#### Вариант A: Настроим csync2 через inetd
```bash
# 1. Добавим конфиг для inetd
ansible gateways -m copy -a "content='30865 stream tcp nowait root /usr/sbin/csync2 csync2 -i' dest=/etc/inetd.conf mode=0640"

# 2. Перезапустим inetd
ansible gateways -m shell -a "systemctl restart openbsd-inetd"
```

#### Вариант B: Изменим порт csync2
```bash
# 1. В конфиге csync2 добавим порт
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# 2. Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"
```

### Проверка после настройки:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (настройка через inetd), так как это стандартный подход в системах с openbsd-inetd. Это обеспечит правильную работу csync2 без конфликтов портов.

--------------------------------------------------------


<br/>

## 5 [ALT]. Установка csync2 посредством ansible с машины 192.168.87.74 (ноут)

## 0. как всегда создаём структуру:
```bash
└─ ~/.ansible/infra-proj $ tree
.
├── ./ansible.cfg
├── ./csync2.conf
├── ./csync2.key
├── ./csync2_ssl_cert.pem
├── ./csync2_ssl_key.pem
├── ./group_vars
├── ./inventory
│   └── ./inventory/hosts.ini
├── ./playbooks
│   └── ./playbooks/play_csync.yml
└── ./roles
    └── ./roles/csync2
        ├── ./roles/csync2/files
        │   └── ./roles/csync2/files/cluster.key
        ├── ./roles/csync2/tasks
        │   └── ./roles/csync2/tasks/main.yml
        └── ./roles/csync2/templates
            └── ./roles/csync2/templates/csync2.cfg.j2
```
<details>
<summary>❗hosts.ini❗</summary>
  
```ini
└─ ~/.ansible/infra-proj $ ccat inventory/hosts.ini 
[gateways]
dmzgateway1 ansible_host=192.168.87.253 ansible_user=root
dmzgateway2 ansible_host=192.168.87.252 ansible_user=root
dmzgateway3 ansible_host=192.168.87.251 ansible_user=root

[proxmox]
prox4 ansible_host=192.168.87.17 ansible_user=root
pmx5 ansible_host=192.168.87.20 ansible_user=root
pmx6 ansible_host=192.168.87.6 ansible_user=root
```
</details>


## **1. Подготовка Ansible Playbook**
Создадим/проверим структуру файлов для развёртывания `csync2`.

### **1.1. Файл `playbooks/play_csync.yml`**
```yaml
---
- name: Install and configure csync2 on gateways
  hosts: gateways
  become: yes
  roles:
    - csync2
```

### **1.2. Роль `roles/csync2/tasks/main.yml`**
```yaml
---
- name: Install csync2 package
  apt:
    name: csync2
    state: present
    update_cache: yes
  when: ansible_os_family == 'Debian'

- name: Create csync2 directories
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: 0755
  loop:
    - /etc/csync2
    - /var/lib/csync2

- name: Copy csync2 config
  template:
    src: csync2.cfg.j2
    dest: /etc/csync2/csync2.cfg
    owner: root
    group: root
    mode: 0644

- name: Copy SSL certificates and keys
  copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    owner: root
    group: root
    mode: 0600
  loop:
    - { src: "../../csync2.key", dest: "/etc/csync2/cluster.key" }
    - { src: "../../csync2_ssl_cert.pem", dest: "/etc/csync2/csync2_ssl_cert.pem" }
    - { src: "../../csync2_ssl_key.pem", dest: "/etc/csync2/csync2_ssl_key.pem" }

- name: Create systemd service for csync2
  template:
    src: csync2.service.j2
    dest: /etc/systemd/system/csync2.service
    owner: root
    group: root
    mode: 0644

- name: Enable and start csync2 service
  systemd:
    name: csync2
    state: started
    enabled: yes
    daemon_reload: yes
```

### **1.3. Шаблон `roles/csync2/templates/csync2.cfg.j2`**
```jinja2
group cluster {
    host {{ ansible_hostname }};
    {% for host in groups['gateways'] %}
    {% if host != inventory_hostname %}
    host {{ host }};
    {% endif %}
    {% endfor %}

    key /etc/csync2/cluster.key;
    ssl_cert /etc/csync2/csync2_ssl_cert.pem;
    ssl_key /etc/csync2/csync2_ssl_key.pem;

    include /etc/csync2/sync.d/*.cfg;
    exclude *~ .*;
}
```

### **1.4. Шаблон `roles/csync2/templates/csync2.service.j2`**
```ini
[Unit]
Description=Cluster Synchronization Tool (csync2)
After=network.target
Requires=network-online.target

[Service]
Type=forking
Environment="CSYNC2_SYSTEM_DIR=/etc/csync2"
ExecStart=/usr/sbin/csync2 -c /etc/csync2/csync2.cfg -ii
PIDFile=/var/run/csync2.pid
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
```

---

## **2. Запуск развёртывания**
```bash
ansible-playbook playbooks/play_csync.yml
```

---

## **3. Проверка работы csync2**
### **3.1. Проверим статус сервиса**
```bash
ansible gateways -m command -a "systemctl status csync2 --no-pager" -b
```

### **3.2. Проверим синхронизацию**
```bash
ansible gateways -m command -a "csync2 -c /etc/csync2/csync2.cfg -Tvv" -b
```

### **3.3. Проверим логи**
```bash
ansible gateways -m command -a "journalctl -u csync2 -n 20 --no-pager" -b
```

---

## **4. Что делать, если есть ошибки?**
### **4.1. Проверить конфиг вручную**
```bash
ansible gateways -m command -a "cat /etc/csync2/csync2.cfg" -b
```

### **4.2. Проверить права на ключи**
```bash
ansible gateways -m command -a "ls -la /etc/csync2/" -b
```

### **4.3. Запустить csync2 в debug-режиме**
```bash
ansible gateways -m command -a "/usr/sbin/csync2 -c /etc/csync2/csync2.cfg -d -vv" -b
```

---

## **5. Дополнительные настройки (опционально)**
### **5.1. Добавление файлов для синхронизации**
Создайте файл `/etc/csync2/sync.d/sync_rules.cfg`:
```text
include /etc/hosts;
include /etc/nginx/conf.d/*.conf;
exclude /etc/csync2/*.key;
```

### **5.2. Принудительная синхронизация**
```bash
ansible gateways -m command -a "csync2 -c /etc/csync2/csync2.cfg -x" -b
```

---

## **Вывод**
Теперь `csync2` должен быть корректно установлен и работать на всех gateways. Если возникнут проблемы – пишите, помогу разобраться! 🚀





