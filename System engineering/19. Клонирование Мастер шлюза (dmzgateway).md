# Создание клонов/кластера машины 102 (dmzgateway) для обеспечения устойчивости сети.


## 1. Установить утилиту keepalived на машину 102 (dmzgateway)
Обновить систему на всякийслучай:
```bash
sudo apt update && sudo apt upgrade -y
```

Установка keepalived
```bash
sudo apt install keepalived -y
```

Проверка статуса
```bash
sudo systemctl status keepalived.service -l --no-pager
```

Запуск и включение в автозагрузку сделаем чуть позже, т.к. в нашем случае потребуется настроить сперва конфигурационный файл `/etc/keepalived/keepalived.conf`, а после сделать копии с оригинальной машины.
```bash
sudo systemctl enable --now keepalived
```
<br/>


## 2. Настройка конфигурационного файла
К сведению: IP адрес = `192.168.87.3` - является **плавающим** для "железных" машин кластера.
Образец можно посмотреть на "железных" машинах [pmx5](https://192.168.87.20:8006/#v1:0:=node%2Fpmx5:4:11::::::), [pmx6](https://192.168.87.17:8006/#v1:0:=node%2Fpmx6:4:11:::::11:), [prox4](https://192.168.87.17:8006/#v1:0:=node%2Fprox4:4:11::::::). 

Планриуется настройка Keepalived для трёх нод (dmzgateway1, dmzgateway2, dmzgateway3).
Схема должна выглядет следующим образом:

Адреса оригинальной машины станут плавающими:
- *`192.168.87.2`*  **eth0** → **(vmbr0):Bridge** 
- *`192.168.46.1`*  **eth1** → **(dmznet):Bridge** 
- *`192.168.45.1`*  **eth2** → **(pgnet):Bridge** 

Физические адреса нод:
| Нода            | vmbr0              | dmznet             | pgnet           |  
|-----------------|--------------------|--------------------|--------------------|  
| **dmzgateway1** | **192.168.87.253** | **192.168.46.253** | **192.168.45.253** |  
| **dmzgateway2** | **192.168.87.252** | **192.168.46.252** | **192.168.45.252** |  
| **dmzgateway3** | **192.168.87.251** | **192.168.46.251** | **192.168.45.251** |  
| **Gateway**     | **192.168.87.1**   | N/A                | N/A             |  


Также читай статью: [Пример использования keepalived на Linux](https://www.dmosk.ru/miniinstruktions.php?mini=keepalived-linux#settings)

### 1. Проверям интерфейсы
```bash
ip -br -c addr show; 
ip -c addr show;
```

### 2. Создаём конф. файл

<details>
<summary>❗keepalived.conf❗</summary>
  
```c
global_defs {
    router_id dmzgateway1
}

vrrp_instance VI_VMBR0 {
    state BACKUP           # Все ноды стартуют как BACKUP
    interface eth0         # Интерфейс для VRRP
    virtual_router_id 51   # Должен быть одинаковым в кластере
    priority 100           # Одинаковый приоритет на всех нодах
    advert_int 1           # Частота объявлений (сек)
    nopreempt              # Запрещает перехват роли мастера
    virtual_ipaddress {
        192.168.87.2/24    # Виртуальный IP
    }
}

vrrp_instance VI_DMZNET {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.46.1/24
    }
}

vrrp_instance VI_PGNET {
    state BACKUP
    interface eth2
    virtual_router_id 53
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.45.1/24
    }
}
```
</details>  

```bash
##/etc/keepalived/{master,backup}.sh
#!/bin/bash
echo "[$(date)] Нода $HOSTNAME перешла в состояние $1" >> /var/log/keepalived-state.log
```

`state BACKUP` и `priority 100` одинаковый на всех машинах, т.к. приоритета нет к машинам после перезагрузки

И проверяем синтаксис:
```bash
keepalived -t
```
Проверяем маршруты
```bash
ip route show
```
Проверка на скрытыие символы в пароле:
```bash
hexdump -C /etc/keepalived/keepalived.conf | grep -A5 "auth_pass"
```
Проверка VIP на одной из нод:
```bash
ip addr show eth0 | grep 192.168.87.2
```
Проверка логов:
```bash
journalctl -u keepalived -f --no-pager
```


Отредактировать данный файл на остальных нодах после клонирования оригинальной машины.
<br/>


## 3. Клонируем машины
Используя GUI, может возникнуть ошибка при клонировании контейнера в ProxMox:
***unable to clone mountpoint 'mp0' (type bind) (500)***

Эта ошибка возникает при попытке клонирования контейнера LXC в Proxmox VE, когда у исходного контейнера есть привязанные (bind) точки монтирования. 

В рамках GUI можно сделать Deattach Mount Point, т.к. в нашем случае он не нужен.

**Причины проблемы:**
- Контейнер имеет bind-монтирования (тип mp0)
- Эти монтирования ссылаются на пути на хосте, которые не существуют или недоступны
- Proxmox не может автоматически обработать такие монтирования при клонировании

**Решение:**
1) Зайти на "железный" сервер, где хранится контейнер и проверить конфигурацию lxc контейнера.
```bash
root@pmx6 ~ > ccat /etc/pve/lxc/102.conf 
arch: amd64
cores: 1
features: fuse=1,mknod=1,nesting=1
hostname: dmzgateway
memory: 1024
mp0: /stg/8tb/share,mp=/shared,backup=0
net0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.87.1,hwaddr=BC:24:11:AD:EC:E2,ip=192.168.87.2/24,type=veth
net1: name=eth1,bridge=dmznet,hwaddr=BC:24:11:E6:5D:BD,ip=192.168.46.1/24,type=veth
net2: name=eth2,bridge=pgnet,firewall=1,hwaddr=BC:24:11:C4:6E:72,ip=192.168.45.1/24,type=veth
onboot: 1
ostype: debian
rootfs: ssd_1tb:102/vm-102-disk-0.raw,size=10G
swap: 0
unprivileged: 1
```
Строку временно до клонирования поменять на:
```bash
mp0: /stg/8tb/share,mp=/shared,backup=0,mountoptions=noatime
```

2) Склонировать с помощью pct.
```bash
vzdump 102
pct restore NEWID /var/lib/vz/dump/vzdump-lxc-102-*.tar.gz
```





















