# Создание клонов/кластера машины 102 (dmzgateway) для обеспечения устойчивости сети.


## 1. Установить утилиту keepalived на машину 102 (dmzgateway)
Обновить систему на всякий случай:
```bash
sudo apt update && sudo apt upgrade -y
```

Установка keepalived
```bash
sudo apt install keepalived -y
```

Проверка статуса
```bash
sudo systemctl status keepalived.service -l --no-pager
```

Запуск и включение в автозагрузку сделаем чуть позже, т.к. в нашем случае потребуется настроить сперва конфигурационный файл `/etc/keepalived/keepalived.conf`, а после сделать копии с оригинальной машины.
```bash
sudo systemctl enable --now keepalived
```
<br/>


## 2. Настройка конфигурационного файла
К сведению: IP адрес = `192.168.87.3` - является **плавающим** для "железных" машин кластера.
Образец можно посмотреть на "железных" машинах [pmx5](https://192.168.87.20:8006/#v1:0:=node%2Fpmx5:4:11::::::), [pmx6](https://192.168.87.17:8006/#v1:0:=node%2Fpmx6:4:11:::::11:), [prox4](https://192.168.87.17:8006/#v1:0:=node%2Fprox4:4:11::::::). 

Планриуется настройка Keepalived для трёх нод (dmzgateway1, dmzgateway2, dmzgateway3).
Схема должна выглядет следующим образом:

Адреса оригинальной машины станут плавающими:
- *`192.168.87.2`*  **eth0** → **(vmbr0):Bridge** 
- *`192.168.46.1`*  **eth1** → **(dmznet):Bridge** 
- *`192.168.45.1`*  **eth2** → **(pgnet):Bridge** 

Физические адреса нод:
| Нода            | vmbr0              | dmznet             | pgnet           |  
|-----------------|--------------------|--------------------|--------------------|  
| **dmzgateway1** | **192.168.87.253** | **192.168.46.253** | **192.168.45.253** |  
| **dmzgateway2** | **192.168.87.252** | **192.168.46.252** | **192.168.45.252** |  
| **dmzgateway3** | **192.168.87.251** | **192.168.46.251** | **192.168.45.251** |  
| **Gateway**     | **192.168.87.1**   | N/A                | N/A             |  


Также читай статью: [Пример использования keepalived на Linux](https://www.dmosk.ru/miniinstruktions.php?mini=keepalived-linux#settings)

### 1. Проверям интерфейсы
```bash
ip -br -c addr show; 
ip -c addr show;
```

### 2. Создаём конф. файл

<details>
<summary>❗keepalived.conf❗</summary>
  
```c
global_defs {
    router_id dmzgateway1
}

vrrp_instance VI_VMBR0 {
    state BACKUP           # Все ноды стартуют как BACKUP
    interface eth0         # Интерфейс для VRRP
    virtual_router_id 51   # Должен быть одинаковым в кластере
    priority 100           # Одинаковый приоритет на всех нодах
    advert_int 1           # Частота объявлений (сек)
    nopreempt              # Запрещает перехват роли мастера
    virtual_ipaddress {
        192.168.87.2/24    # Виртуальный IP
    }
}

vrrp_instance VI_DMZNET {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.46.1/24
    }
}

vrrp_instance VI_PGNET {
    state BACKUP
    interface eth2
    virtual_router_id 53
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.45.1/24
    }
}
```
</details>  

```bash
##/etc/keepalived/{master,backup}.sh
#!/bin/bash
echo "[$(date)] Нода $HOSTNAME перешла в состояние $1" >> /var/log/keepalived-state.log
```

`state BACKUP` и `priority 100` одинаковый на всех машинах, т.к. приоритета нет к машинам после перезагрузки

И проверяем синтаксис:
```bash
keepalived -t
```
Проверяем маршруты
```bash
ip route show
```
Проверка на скрытыие символы в пароле:
```bash
hexdump -C /etc/keepalived/keepalived.conf | grep -A5 "auth_pass"
```
Проверка VIP на одной из нод:
```bash
ip addr show eth0 | grep 192.168.87.2
```
Проверка логов:
```bash
journalctl -u keepalived -f --no-pager
```

Отредактировать данный файл на остальных нодах после клонирования оригинальной машины.
<br/>



## 3. Клонируем машины
Используя GUI, может возникнуть ошибка при клонировании контейнера в ProxMox:
***unable to clone mountpoint 'mp0' (type bind) (500)***

Эта ошибка возникает при попытке клонирования контейнера LXC в Proxmox VE, когда у исходного контейнера есть привязанные (bind) точки монтирования. 

В рамках GUI можно сделать Deattach Mount Point, т.к. в нашем случае он не нужен.

**Причины проблемы:**
- Контейнер имеет bind-монтирования (тип mp0)
- Эти монтирования ссылаются на пути на хосте, которые не существуют или недоступны
- Proxmox не может автоматически обработать такие монтирования при клонировании

**Решение:**
1) Зайти на "железный" сервер, где хранится контейнер и проверить конфигурацию lxc контейнера.
```bash
root@pmx6 ~ > ccat /etc/pve/lxc/102.conf 
arch: amd64
cores: 1
features: fuse=1,mknod=1,nesting=1
hostname: dmzgateway
memory: 1024
mp0: /stg/8tb/share,mp=/shared,backup=0
net0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.87.1,hwaddr=BC:24:11:AD:EC:E2,ip=192.168.87.2/24,type=veth
net1: name=eth1,bridge=dmznet,hwaddr=BC:24:11:E6:5D:BD,ip=192.168.46.1/24,type=veth
net2: name=eth2,bridge=pgnet,firewall=1,hwaddr=BC:24:11:C4:6E:72,ip=192.168.45.1/24,type=veth
onboot: 1
ostype: debian
rootfs: ssd_1tb:102/vm-102-disk-0.raw,size=10G
swap: 0
unprivileged: 1
```
Строку временно до клонирования поменять на:
```bash
mp0: /stg/8tb/share,mp=/shared,backup=0,mountoptions=noatime
```

2) Склонировать с помощью pct.
```bash
vzdump 102
pct restore NEWID /var/lib/vz/dump/vzdump-lxc-102-*.tar.gz
```
<br/>



## 4. Установка и настройка csync2 через Ad-Hoc Ansible команды
Создаём ansible.cfg файлик:
```cfg
[defaults]
inventory = ./inventory/hosts.ini        # либо ~/.ansible/infra-proj/inventory/hosts.ini
private_key_file = ~/.ssh/id_rsa         # Путь к вашему приватному ключу
host_key_checking = False                # Отключает проверку SSH-ключей хостов
interpreter_python = /usr/bin/python3    # какой интерпретатор Python использовать на управляемых узлах.
```
### 1. Проверка подключения.
```bash
└─ ~/.ansible/infra-proj $ ansible gateways -m ping
dmzgateway2 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway1 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway3 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

### 2. Добавим записи в /etc/hosts на всех узлах и проверим
```bash
ansible gateways -m blockinfile -a "path=/etc/hosts block='192.168.87.253 dmzgateway1
192.168.87.252 dmzgateway2
192.168.87.251 dmzgateway3'"
```
```bash
ansible gateways -m shell -a "ping -c 2 dmzgateway1"
ansible gateways -m shell -a "ping -c 2 dmzgateway2"
ansible gateways -m shell -a "ping -c 2 dmzgateway3"
```

### 2. Установка пакета csync2 на все шлюзы
```bash
ansible gateways -m apt -a "name=csync2 state=present update_cache=yes"
# или:
ansible gateways -i hosts -u root -k -m apt -a "name=csync2 state=present update_cache=yes"
```

### 3. Создание конфигурационных каталогов
```bash
ansible gateways -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -m file -a "path=/etc/csync2/key.d state=directory mode=0750"

# или:
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2/key.d state=directory mode=0750"
```

### 4.1. Генерация ключа на dmzgateway1 и копирование на остальные узлы
```bash
# Генерация ключа на dmzgateway1
ansible dmzgateway1 -i inventory/hosts.ini  -m shell -a "csync2 -k /tmp/csync2.key"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m shell -a "csync2 -k /tmp/csync2.key"

# Копирование с dmzgateway1 на локальную машину
ansible dmzgateway1 -i inventory/hosts.ini -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"

# Распространение ключа на все узлы
ansible gateways -i inventory/hosts.ini -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
```

### 4.2. Генерация ключа локальной машине (192.168.87.74) и копирование на остальные узлы
```bash
# Генерация ключа на локальной машине
csync2 -k ./csync2.key

# если Ximper Linux, то будет
sudo find /usr -name csync2
whereis csync2
/usr/sbin/csync2 -k ./csync2.key
chmod 600 ./csync2.key

# Копирование ключа на все узлы
ansible gateways -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
#---------------------------------------------------
```


### 4. Создание конфигурационного файла `/etc/csync2/csync2.cfg`

```bash
ansible gateways -m copy -a "content='group cluster {
    host (dmzgateway1(192.168.87.251) dmzgateway2(192.168.87.252) dmzgateway3(192.168.87.253));
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec \"/usr/sbin/service keepalived restart\";
        logfile \"/var/log/csync2_action.log\";
    }
    backup_directory /var/lib/csync2;
    auto younger;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```


### 5. Включение и запуск службы
```bash
ansible gateways -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -m shell -a "systemctl enable csync2 --now"
ansible gateways -m shell -a "systemctl status csync2"
```

### 6.1. Инициализация синхронизации
```bash
ansible dmzgateway1 -m shell -a "csync2 -xv"
ansible dmzgateway1 -m shell -a "csync2 -N 192.168.87.251 -xvv"
# или:
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 6.2. Тест синхронизации
```bash
# Создаем тестовый файл
ansible dmzgateway1 -m copy -a "content='test sync' dest=/etc/keepalived/test_sync.txt"

# Запускаем синхронизацию
ansible dmzgateway1 -m shell -a "csync2 -x"

# Проверяем на всех узлах
ansible gateways -m shell -a "ls -la /etc/keepalived/ | grep test_sync"
```

### 7. Проверка статуса
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ls -la /etc/csync2"
# или:
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```

### 8. Дополнительные проверки
```bash
# Проверка логов
ansible gateways -m shell -a "tail -n 20 /var/log/csync2*"

# Проверка подключений
ansible gateways -m shell -a "netstat -tulpn | grep csync2"
```

### 9. Создание SSL-сертификатов

Можно руками на мастер ноде руками создать и руками расзложить по нодам:
```bash
openssl req -x509 -newkey rsa:4096 -keyout csync2_ssl_key.pem -out csync2_ssl_cert.pem -days 3650 -nodes -subj "/CN=dmzgateway1";
# раскидываем:
for node in dmzgateway2 dmzgateway3; do
  scp /etc/csync2_ssl_{key,cert}.pem root@$node:/etc/
done
```
Либо попробуем развернуть сертификаты на все узлы через Ansible:
```bash
# Копируем ключ и сертификат
ansible gateways -m copy -a "src=csync2_ssl_key.pem dest=/etc/csync2_ssl_key.pem mode=0600"
ansible gateways -m copy -a "src=csync2_ssl_cert.pem dest=/etc/csync2_ssl_cert.pem mode=0644"

# Альтернативно, можно создать сертификаты прямо на узлах (если есть openssl):
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2_ssl_key.pem -out /etc/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
ansible gateways -m shell -a "chmod 600 /etc/csync2_ssl_key.pem; chmod 644 /etc/csync2_ssl_cert.pem"
```

Ad-Hoc командой создаём сертификаты на всех узлах:
```bash
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2_ssl_key.pem -out /etc/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
```
Права:
```bash
ansible gateways -m shell -a "chmod 600 /etc/csync2_ssl_key.pem; chmod 644 /etc/csync2_ssl_cert.pem"
```


Обновляем конфигурацию **csync2**:
```conf
ansible gateways -m copy -a "content='group cluster {
    host (dmzgateway1 dmzgateway2 dmzgateway3);
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    ssl_cert /etc/csync2_ssl_cert.pem;
    ssl_key /etc/csync2_ssl_key.pem;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
```bash
ansible gateways -m shell -a "systemctl restart csync2"
ansible gateways -m shell -a "csync2 -xv"
```



```bash
ansible gateways -m shell -a "grep host /etc/csync2/csync2.cfg"
```

### 10. Настройка автоматической синхронизации (опционально)
```bash
# Синхронизация Каждые 5 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/5' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 10 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/10' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 30 минут
ansible gateways -m cron -a "name='30-min csync2 sync' minute='*/30' job='/usr/sbin/csync2 -x'"

# Синхронизация каждый час (в 0 минут каждого часа)
ansible gateways -m cron -a "name='Hourly csync2 sync' minute='0' job='/usr/sbin/csync2 -x'"

# Синхронизация каждые 4 часа (в 0 минут каждые 4 часа)
ansible gateways -m cron -a "name='4-hour csync2 sync' minute='0' hour='*/4' job='/usr/sbin/csync2 -x'"

## Удаление cron-заданий (если нужно)
# Удалить ежечасную синхронизацию
ansible gateways -m cron -a "name='Hourly csync2 sync' state=absent"

# Удалить синхронизацию каждые 4 часа
ansible gateways -m cron -a "name='4-hour csync2 sync' state=absent"
```
Проверка cron триггеров:
```
ansible gateways -m shell -a "crontab -l | grep csync2"
```


### Ключевые особенности:
1. Все команды используют SSH-ключ (как настроено в ansible.cfg)
2. Ключ генерируется один раз локально
3. Конфигурация применяется одинаково на всех узлах
4. Не требуется ввод паролей
5. Для аутентификации используется пароль root (`-k` флаг)
6. Ключ генерируется один раз на dmzgateway1 и распространяется на остальные узлы

### Дополнительные проверки:
```bash
# Проверка синхронизированных файлов
ansible gateways -i hosts -u root -k -m shell -a "ls -la /etc/csync2"

# Проверка логов
ansible gateways -i hosts -u root -k -m shell -a "tail -n 20 /var/log/csync2*"
```

--------------------------------------------------------
### Если xinetd уже занимает порт 30865:
```bash
# Проверка порта
ansible gateways -m shell -a "netstat -tulpn | grep 30865 || ss -tulpn | grep 30865"

# Остановим xinetd (так как csync2 должен работать самостоятельно)
ansible gateways -m shell -a "systemctl stop xinetd && systemctl disable xinetd"

# Убедимся, что порт освободился
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```
```bash
# Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"

# Проверим статус
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2 || echo 'csync2 not listening'"

# Инициализируем синхронизацию
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xvv"
```

#### Вариант A: Настроим csync2 работать через xinetd
```bash
# 1. Включим xinetd обратно
ansible gateways -m shell -a "systemctl enable xinetd --now"

# 2. Настроим конфиг xinetd для csync2
ansible gateways -m copy -a "content='service csync2
{
    disable = no
    socket_type = stream
    protocol = tcp
    wait = no
    user = root
    server = /usr/sbin/csync2
    server_args = -i
    only_from = 192.168.87.0/24
}' dest=/etc/xinetd.d/csync2 mode=0640"

# 3. Перезапустим xinetd
ansible gateways -m shell -a "systemctl restart xinetd"
```

#### Вариант B: Изменим порт csync2 (если нужно оставить xinetd для других сервисов)
```bash
# В конфиге /etc/csync2/csync2.cfg добавьте:
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# Затем перезапустите:
ansible gateways -m shell -a "systemctl restart csync2"
```

#### Важно:
1. Выберите только один вариант (A или B)
2. После настройки проверьте:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (использовать xinetd), так как это стандартный подход в ALT Linux для csync2.

--------------------------------------------------------
### Если openbsd-inetd (простой inetd) уже занимает порт 30865:
### 1. Остановим inetd и освободим порт 30865:
```bash
ansible gateways -m shell -a "systemctl stop openbsd-inetd && systemctl disable openbsd-inetd"
```

### 2. Проверим освобождение порта:
```bash
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```

### 3. Перезапустим csync2:
```bash
ansible gateways -m shell -a "systemctl restart csync2"
```

### 4. Проверим работу csync2:
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2"
```

### 5. Инициализируем синхронизацию:
```bash
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xv"
```

### Если нужно оставить inetd для других сервисов:

#### Вариант A: Настроим csync2 через inetd
```bash
# 1. Добавим конфиг для inetd
ansible gateways -m copy -a "content='30865 stream tcp nowait root /usr/sbin/csync2 csync2 -i' dest=/etc/inetd.conf mode=0640"

# 2. Перезапустим inetd
ansible gateways -m shell -a "systemctl restart openbsd-inetd"
```

#### Вариант B: Изменим порт csync2
```bash
# 1. В конфиге csync2 добавим порт
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# 2. Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"
```

### Проверка после настройки:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (настройка через inetd), так как это стандартный подход в системах с openbsd-inetd. Это обеспечит правильную работу csync2 без конфликтов портов.

--------------------------------------------------------


<br/>

## 5 [ALT]. Установка csync2 посредством ansible с машины 192.168.87.74 (ноут)

### 1. как всегда создаём структуру:
```bash
└─ ~/.ansible/infra-proj $ tree -f
.
├── ./ansible.cfg
├── ./group_vars
├── ./inventory
│   └── ./inventory/hosts.ini
├── ./playbooks
│   └── ./playbooks/play_csync.yml
└── ./roles
```
<details>
<summary>❗hosts.ini and playbook❗</summary>
  
```ini
└─ ~/.ansible/infra-proj $ ccat inventory/hosts.ini 
[gateways]
dmzgateway1 ansible_host=192.168.87.253 ansible_user=root
dmzgateway2 ansible_host=192.168.87.252 ansible_user=root
dmzgateway3 ansible_host=192.168.87.251 ansible_user=root

[proxmox]
prox4 ansible_host=192.168.87.17 ansible_user=root
pmx5 ansible_host=192.168.87.20 ansible_user=root
pmx6 ansible_host=192.168.87.6 ansible_user=root
```
```yaml
└─ ~/.ansible/infra-proj $ ccat playbooks/play_csync.yml 
---
- name: Установка и настройка csync2
  hosts: gateways
  become: yes
  vars:
    csync2_key_path: "/etc/csync2/key.d/csync2.key"
    csync2_config_path: "/etc/csync2/csync2.cfg"
  
  tasks:
    - name: Установка csync2
      apt:
        name: csync2
        state: present
        update_cache: yes

    - name: Создание конфигурационных каталогов
      file:
        path: "{{ item }}"
        state: directory
        mode: 0750
      loop:
        - /etc/csync2
        - /etc/csync2/key.d

    - name: Копирование ключа аутентификации
      copy:
        src: "{{ playbook_dir }}/roles/csync2/files/csync2.key"
        dest: "{{ csync2_key_path }}"
        mode: 0600

    - name: Настройка конфигурации csync2
      template:
        src: "{{ playbook_dir }}/roles/csync2/templates/csync2.cfg.j2"
        dest: "{{ csync2_config_path }}"
        mode: 0640
      notify: restart csync2

  handlers:
    - name: restart csync2
      service:
        name: csync2
        state: restarted

## playbook_dir - это встроенная переменная Ansible, которая автоматически указывает на директорию с выполняемым плейбуком. Её можно использовать без объявления.
```
</details>


### 2. Подготавливаем файлы конфигурации
Создаем файл `~/.ansible/infra-proj/roles/csync2/files/csync2.cfg`:
```cfg
group cluster
{
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key /etc/csync2/key.d/csync2.key;

    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;

    action
    {
        pattern /etc/keepalived/keepalived.conf;
        exec "/usr/sbin/service keepalived restart";
        logfile "/var/log/csync2_action.log";
    }

    backup_directory /var/lib/csync2;
    auto younger;
}
```
Создаем файл `{{ playbook_dir }}/roles/csync2/templates/csync2.cfg.j2`:
```jinja
group cluster
{
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key {{ csync2_key_path }};

    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;

    action
    {
        pattern /etc/keepalived/keepalived.conf;
        exec "/usr/sbin/service keepalived restart";
        logfile "/var/log/csync2_action.log";
    }

    backup_directory /var/lib/csync2;
    auto younger;
}
```


Генерируем ключ (на dmzgateway1):
```bash
mkdir -p ~/.ansible/infra-proj/roles/csync2/files
csync2 -k ~/.ansible/infra-proj/roles/csync2/files/csync2.key
```

### 3. Запуск плейбука и инициализация
```bash
ansible-playbook -i ~/.ansible/infra-proj/inventory/hosts.ini play_csync.yml -u root -k
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 4. Доп. настройки
Для автоматического запуска csync2:
```bash
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
```
Для проверки статуса:
```bash
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```





