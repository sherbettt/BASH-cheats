# Создание клонов/кластера машины 102 (dmzgateway) для обеспечения устойчивости сети.


## 1. Установить утилиту keepalived на машину 102 (dmzgateway)
Обновить систему на всякий случай:
```bash
sudo apt update && sudo apt upgrade -y
```

Установка keepalived
```bash
sudo apt install keepalived -y
```

Проверка статуса
```bash
sudo systemctl status keepalived.service -l --no-pager
```

Запуск и включение в автозагрузку сделаем чуть позже, т.к. в нашем случае потребуется настроить сперва конфигурационный файл `/etc/keepalived/keepalived.conf`, а после сделать копии с оригинальной машины.
```bash
sudo systemctl enable --now keepalived
```
<br/>


## 2. Настройка конфигурационного файла
К сведению: IP адрес = `192.168.87.3` - является **плавающим** для "железных" машин кластера.
Образец можно посмотреть на "железных" машинах [pmx5](https://192.168.87.20:8006/#v1:0:=node%2Fpmx5:4:11::::::), [pmx6](https://192.168.87.17:8006/#v1:0:=node%2Fpmx6:4:11:::::11:), [prox4](https://192.168.87.17:8006/#v1:0:=node%2Fprox4:4:11::::::). 

Планриуется настройка Keepalived для трёх нод (dmzgateway1, dmzgateway2, dmzgateway3).
Схема должна выглядет следующим образом:

Адреса оригинальной машины станут плавающими:
- *`192.168.87.2`*  **eth0** → **(vmbr0):Bridge** 
- *`192.168.46.1`*  **eth1** → **(dmznet):Bridge** 
- *`192.168.45.1`*  **eth2** → **(pgnet):Bridge** 

Физические адреса нод:
| Нода            | vmbr0              | dmznet             | pgnet           |  
|-----------------|--------------------|--------------------|--------------------|  
| **dmzgateway1** | **192.168.87.253** | **192.168.46.253** | **192.168.45.253** |  
| **dmzgateway2** | **192.168.87.252** | **192.168.46.252** | **192.168.45.252** |  
| **dmzgateway3** | **192.168.87.251** | **192.168.46.251** | **192.168.45.251** |  
| **Gateway**     | **192.168.87.1**   | N/A                | N/A             |  


Также читай статью: [Пример использования keepalived на Linux](https://www.dmosk.ru/miniinstruktions.php?mini=keepalived-linux#settings)

### 1. Проверям интерфейсы
```bash
ip -br -c addr show; 
ip -c addr show;
```

### 2. Создаём конф. файл

<details>
<summary>❗keepalived.conf❗</summary>
  
```c
global_defs {
    router_id dmzgateway1
}

vrrp_instance VI_VMBR0 {
    state BACKUP           # Все ноды стартуют как BACKUP
    interface eth0         # Интерфейс для VRRP
    virtual_router_id 51   # Должен быть одинаковым в кластере
    priority 100           # Одинаковый приоритет на всех нодах
    advert_int 1           # Частота объявлений (сек)
    nopreempt              # Запрещает перехват роли мастера
    virtual_ipaddress {
        192.168.87.2/24    # Виртуальный IP
    }
}

vrrp_instance VI_DMZNET {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.46.1/24
    }
}

vrrp_instance VI_PGNET {
    state BACKUP
    interface eth2
    virtual_router_id 53
    priority 100
    advert_int 1
    nopreempt
    virtual_ipaddress {
        192.168.45.1/24
    }
}
```
</details>  

```bash
##/etc/keepalived/{master,backup}.sh
#!/bin/bash
echo "[$(date)] Нода $HOSTNAME перешла в состояние $1" >> /var/log/keepalived-state.log
```

`state BACKUP` и `priority 100` одинаковый на всех машинах, т.к. приоритета нет к машинам после перезагрузки

И проверяем синтаксис:
```bash
keepalived -t
```
Проверяем маршруты
```bash
ip route show
```
Проверка на скрытыие символы в пароле:
```bash
hexdump -C /etc/keepalived/keepalived.conf | grep -A5 "auth_pass"
```
Проверка VIP на одной из нод:
```bash
ip addr show eth0 | grep 192.168.87.2
```
Проверка логов:
```bash
journalctl -u keepalived -f --no-pager
```


Отредактировать данный файл на остальных нодах после клонирования оригинальной машины.
<br/>


## 3. Клонируем машины
Используя GUI, может возникнуть ошибка при клонировании контейнера в ProxMox:
***unable to clone mountpoint 'mp0' (type bind) (500)***

Эта ошибка возникает при попытке клонирования контейнера LXC в Proxmox VE, когда у исходного контейнера есть привязанные (bind) точки монтирования. 

В рамках GUI можно сделать Deattach Mount Point, т.к. в нашем случае он не нужен.

**Причины проблемы:**
- Контейнер имеет bind-монтирования (тип mp0)
- Эти монтирования ссылаются на пути на хосте, которые не существуют или недоступны
- Proxmox не может автоматически обработать такие монтирования при клонировании

**Решение:**
1) Зайти на "железный" сервер, где хранится контейнер и проверить конфигурацию lxc контейнера.
```bash
root@pmx6 ~ > ccat /etc/pve/lxc/102.conf 
arch: amd64
cores: 1
features: fuse=1,mknod=1,nesting=1
hostname: dmzgateway
memory: 1024
mp0: /stg/8tb/share,mp=/shared,backup=0
net0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.87.1,hwaddr=BC:24:11:AD:EC:E2,ip=192.168.87.2/24,type=veth
net1: name=eth1,bridge=dmznet,hwaddr=BC:24:11:E6:5D:BD,ip=192.168.46.1/24,type=veth
net2: name=eth2,bridge=pgnet,firewall=1,hwaddr=BC:24:11:C4:6E:72,ip=192.168.45.1/24,type=veth
onboot: 1
ostype: debian
rootfs: ssd_1tb:102/vm-102-disk-0.raw,size=10G
swap: 0
unprivileged: 1
```
Строку временно до клонирования поменять на:
```bash
mp0: /stg/8tb/share,mp=/shared,backup=0,mountoptions=noatime
```

2) Склонировать с помощью pct.
```bash
vzdump 102
pct restore NEWID /var/lib/vz/dump/vzdump-lxc-102-*.tar.gz
```
<br/>


## 4. Установка и настройка csync2 через Ad-Hoc Ansible команды
Создаём ansible.cfg файлик:
```cfg
[defaults]
inventory = ./inventory/hosts.ini        # либо ~/.ansible/infra-proj/inventory/hosts.ini
private_key_file = ~/.ssh/id_rsa         # Путь к вашему приватному ключу
host_key_checking = False                # Отключает проверку SSH-ключей хостов
interpreter_python = /usr/bin/python3    # какой интерпретатор Python использовать на управляемых узлах.
```
### 1. Проверка подключения.
```bash
└─ ~/.ansible/infra-proj $ ansible gateways -m ping
dmzgateway2 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway1 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway3 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

### 1. Установка пакета csync2 на все шлюзы
```bash
ansible gateways -m apt -a "name=csync2 state=present update_cache=yes"
# или:
ansible gateways -i hosts -u root -k -m apt -a "name=csync2 state=present update_cache=yes"
```

### 2. Создание конфигурационных каталогов
```bash
ansible gateways -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -m file -a "path=/etc/csync2/key.d state=directory mode=0750"

# или:
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2/key.d state=directory mode=0750"
```

### 3. Генерация ключа на dmzgateway1 и копирование на остальные узлы
```bash
# Генерация ключа на локальной машине
csync2 -k ./csync2.key

# Копирование ключа на все узлы
ansible gateways -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
#---------------------------------------------------

# Генерация ключа
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m shell -a "csync2 -k /tmp/csync2.key"

# Копирование с dmzgateway1 на локальную машину
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"

# Распространение ключа на все узлы
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
```

### 4. Создание конфигурационного файла

```bash
ansible gateways -m copy -a "content='group cluster {
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec \"/usr/sbin/service keepalived restart\";
        logfile \"/var/log/csync2_action.log\";
    }
    backup_directory /var/lib/csync2;
    auto younger;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
Или:

<details>
<summary>❗Вариант2❗</summary>
  
```bash
ansible gateways -i hosts -u root -k -m copy -a "content='group cluster {
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key /etc/csync2/key.d/csync2.key;

    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;

    action {
        pattern /etc/keepalived/keepalived.conf;
        exec \"/usr/sbin/service keepalived restart\";
        logfile \"/var/log/csync2_action.log\";
    }

    backup_directory /var/lib/csync2;
    auto younger;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
</details>

### 5. Включение и запуск службы
```bash
ansible gateways -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
```

### 6. Инициализация синхронизации
```bash
ansible dmzgateway1 -m shell -a "csync2 -xv"
# или:
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 7. Проверка статуса
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ls -la /etc/csync2"
# или:
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```

### Ключевые особенности:
1. Все команды используют SSH-ключ (как настроено в ansible.cfg)
2. Ключ генерируется один раз локально
3. Конфигурация применяется одинаково на всех узлах
4. Не требуется ввод паролей
5. Для аутентификации используется пароль root (`-k` флаг)
6. Ключ генерируется один раз на dmzgateway1 и распространяется на остальные узлы

### Дополнительные проверки:
```bash
# Проверка синхронизированных файлов
ansible gateways -i hosts -u root -k -m shell -a "ls -la /etc/csync2"

# Проверка логов
ansible gateways -i hosts -u root -k -m shell -a "tail -n 20 /var/log/csync2*"
```
<br/>



## 5 [ALT]. Установка csync2 посредством ansible с машины 192.168.87.74 (ноут)

### 1. как всегда создаём структуру:
```bash
└─ ~/.ansible/infra-proj $ tree -f
.
├── ./ansible.cfg
├── ./group_vars
├── ./inventory
│   └── ./inventory/hosts.ini
├── ./playbooks
│   └── ./playbooks/play_csync.yml
└── ./roles
```
<details>
<summary>❗hosts.ini and playbook❗</summary>
  
```ini
└─ ~/.ansible/infra-proj $ ccat inventory/hosts.ini 
[gateways]
dmzgateway1 ansible_host=192.168.87.253 ansible_user=root
dmzgateway2 ansible_host=192.168.87.252 ansible_user=root
dmzgateway3 ansible_host=192.168.87.251 ansible_user=root

[proxmox]
prox4 ansible_host=192.168.87.17 ansible_user=root
pmx5 ansible_host=192.168.87.20 ansible_user=root
pmx6 ansible_host=192.168.87.6 ansible_user=root
```
```yaml
└─ ~/.ansible/infra-proj $ ccat playbooks/play_csync.yml 
---
- name: Установка и настройка csync2
  hosts: gateways
  become: yes
  vars:
    csync2_key_path: "/etc/csync2/key.d/csync2.key"
    csync2_config_path: "/etc/csync2/csync2.cfg"
  
  tasks:
    - name: Установка csync2
      apt:
        name: csync2
        state: present
        update_cache: yes

    - name: Создание конфигурационных каталогов
      file:
        path: "{{ item }}"
        state: directory
        mode: 0750
      loop:
        - /etc/csync2
        - /etc/csync2/key.d

    - name: Копирование ключа аутентификации
      copy:
        src: "{{ playbook_dir }}/roles/csync2/files/csync2.key"
        dest: "{{ csync2_key_path }}"
        mode: 0600

    - name: Настройка конфигурации csync2
      template:
        src: "{{ playbook_dir }}/roles/csync2/templates/csync2.cfg.j2"
        dest: "{{ csync2_config_path }}"
        mode: 0640
      notify: restart csync2

  handlers:
    - name: restart csync2
      service:
        name: csync2
        state: restarted

## playbook_dir - это встроенная переменная Ansible, которая автоматически указывает на директорию с выполняемым плейбуком. Её можно использовать без объявления.
```
</details>


### 2. Подготавливаем файлы конфигурации
Создаем файл `~/.ansible/infra-proj/roles/csync2/files/csync2.cfg`:
```cfg
group cluster
{
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key /etc/csync2/key.d/csync2.key;

    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;

    action
    {
        pattern /etc/keepalived/keepalived.conf;
        exec "/usr/sbin/service keepalived restart";
        logfile "/var/log/csync2_action.log";
    }

    backup_directory /var/lib/csync2;
    auto younger;
}
```
Создаем файл `{{ playbook_dir }}/roles/csync2/templates/csync2.cfg.j2`:
```jinja
group cluster
{
    host dmzgateway1 dmzgateway2 dmzgateway3;
    key {{ csync2_key_path }};

    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;

    action
    {
        pattern /etc/keepalived/keepalived.conf;
        exec "/usr/sbin/service keepalived restart";
        logfile "/var/log/csync2_action.log";
    }

    backup_directory /var/lib/csync2;
    auto younger;
}
```


Генерируем ключ (на dmzgateway1):
```bash
mkdir -p ~/.ansible/infra-proj/roles/csync2/files
csync2 -k ~/.ansible/infra-proj/roles/csync2/files/csync2.key
```

### 3. Запуск плейбука и инициализация
```bash
ansible-playbook -i ~/.ansible/infra-proj/inventory/hosts.ini play_csync.yml -u root -k
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 4. Доп. настройки
Для автоматического запуска csync2:
```bash
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
```
Для проверки статуса:
```bash
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```





