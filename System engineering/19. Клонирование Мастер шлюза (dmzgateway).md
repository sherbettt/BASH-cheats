# Создание клонов/кластера машины 102 (dmzgateway) для обеспечения устойчивости сети

## 1. Установить утилиту keepalived на машину 102 (dmzgateway)
Обновить систему на всякий случай:
```bash
sudo apt update && sudo apt upgrade -y
```

Установка keepalived
```bash
sudo apt install keepalived -y
```

Проверка статуса
```bash
sudo systemctl status keepalived.service -l --no-pager
```

Запуск и включение в автозагрузку сделаем чуть позже, т.к. в нашем случае потребуется настроить сперва конфигурационный файл `/etc/keepalived/keepalived.conf`, а после сделать копии с оригинальной машины.
```bash
sudo systemctl enable --now keepalived
```
<br/>


## 2. Настройка конфигурационного файла
К сведению: IP адрес = `192.168.87.2` - является **плавающим** для "железных" машин кластера.
Образец можно посмотреть на "железных" машинах [pmx5](https://192.168.87.20:8006/#v1:0:=node%2Fpmx5:4:11::::::), [pmx6](https://192.168.87.17:8006/#v1:0:=node%2Fpmx6:4:11:::::11:), [prox4](https://192.168.87.17:8006/#v1:0:=node%2Fprox4:4:11::::::). 

Планируется настройка Keepalived для трёх нод (dmzgateway1, dmzgateway2, dmzgateway3).
Схема должна выглядеть следующим образом:

Адреса оригинальной машины станут плавающими:
- *`192.168.87.2`*  **eth0** → **(vmbr0):Bridge** 
- *`192.168.46.1`*  **eth1** → **(dmznet):Bridge** 
- *`192.168.45.1`*  **eth2** → **(pgnet):Bridge** 

Физические адреса нод:
| Нода            | vmbr0              | dmznet             | pgnet           |  
|-----------------|--------------------|--------------------|--------------------|  
| **dmzgateway1** | **192.168.87.253** | **192.168.46.253** | **192.168.45.253** |  
| **dmzgateway2** | **192.168.87.252** | **192.168.46.252** | **192.168.45.252** |  
| **dmzgateway3** | **192.168.87.251** | **192.168.46.251** | **192.168.45.251** |  
| **dmzgateway**  | **192.168.87.2**   | **192.168.46.1**   | **192.168.45.1**   |  

Также читай статью: [Пример использования keepalived на Linux](https://www.dmosk.ru/miniinstruktions.php?mini=keepalived-linux#settings)

### 1. Проверяем интерфейсы
```bash
ip -br -c addr show; 
ip -c addr show;
```

### 2. Создаём конф. файл

<details>
<summary>❗keepalived.conf❗</summary>

```cfg
global_defs {
    router_id dmzgateway1                    # Уникальный идентификатор ноды
}

vrrp_instance VI_VMBR0 {
    state BACKUP                             # Все ноды стартуют как BACKUP для автоматического переключения
    interface eth0                           # Интерфейс для VRRP
    virtual_router_id 61                     # Уникальный ID (не конфликтует с другими VRRP в сети)
    priority 150                             # Приоритет: 150 (мастер), 120, 100 (бэкапы)
    advert_int 1                             # Частота объявлений (сек)
    #nopreempt              				# Запрещает перехват роли мастера

    # Notify скрипты - выполняются при смене состояния. Их добавлять НЕОБЯЗАТЕЛЬНО
    notify_master "/etc/keepalived/master.sh VI_VMBR0"    # При переходе в MASTER
    notify_backup "/etc/keepalived/backup.sh VI_VMBR0"    # При переходе в BACKUP  
    notify_fault "/etc/keepalived/backup.sh VI_VMBR0"     # При ошибках (переходим в BACKUP)
    
    authentication {                         # Аутентификация между нодами
        auth_type PASS                       # Тип аутентификации - пароль
        auth_pass 1111                       # Одинаковый пароль на всех нодах для этого VRRP instance
		#auth_type NONE
    }
    virtual_ipaddress {
        192.168.87.2/24                      # Виртуальный IP
    }
}

vrrp_instance VI_DMZNET {
    state BACKUP
    interface eth1
    virtual_router_id 62                     # Уникальный ID для второго интерфейса
    priority 150
    advert_int 1
	#nopreempt
    
    # Notify скрипты для второго интерфейса
    notify_master "/etc/keepalived/master.sh VI_DMZNET"
    notify_backup "/etc/keepalived/backup.sh VI_DMZNET"
    notify_fault "/etc/keepalived/backup.sh VI_DMZNET"
    
    authentication {
        auth_type PASS
        auth_pass 2222                       # Пароль для второго VRRP instance
    }
    virtual_ipaddress {
        192.168.46.1/24
    }
}

vrrp_instance VI_PGNET {
    state BACKUP
    interface eth2
    virtual_router_id 63                     # Уникальный ID для третьего интерфейса
    priority 150
    advert_int 1
	#nopreempt
    
    # Notify скрипты для третьего интерфейса
    notify_master "/etc/keepalived/master.sh VI_PGNET"
    notify_backup "/etc/keepalived/backup.sh VI_PGNET"
    notify_fault "/etc/keepalived/backup.sh VI_PGNET"
    
    authentication {
        auth_type PASS
        auth_pass 3333                       # Пароль для третьего VRRP instance
    }
    virtual_ipaddress {
        192.168.45.1/24
    }
}
```
</details>

### 3. Notify скрипты для логирования состояний
Создаем скрипты, которые выполняются при смене состояния VRRP:

**`/etc/keepalived/master.sh`** - выполняется когда нода становится MASTER
**`/etc/keepalived/backup.sh`** - выполняется когда нода становится BACKUP

```bash
#!/bin/bash
echo "[$(date)] Нода $HOSTNAME перешла в состояние $1" | tee -a /var/log/keepalived-state.log
```

Чтобы использовать в keepalived, добавить в конфиг:
```bash
vrrp_instance VI_VMBR0 {
    # ...
    notify_master "/etc/keepalived/master.sh"
    notify_backup "/etc/keepalived/backup.sh" 
}
```

### 4. Критически важные настройки для корректной работы

**На dmzgateway1 (мастер):**
```bash
priority 150    # Высший приоритет
```

**На dmzgateway2 (бэкап):**
```bash  
priority 120    # Средний приоритет
```

**На dmzgateway3 (бэкап):**
```bash
priority 100    # Низший приоритет
```

**Важно:** 
- **Убрать `nopreempt`** - чтобы ноды с высшим приоритетом могли стать мастерами
- **`state BACKUP` на всех нодах** - для автоматического переключения
- **Уникальные `virtual_router_id`** (61, 62, 63) - чтобы избежать конфликтов с другими VRRP в сети
- **Одинаковые пароли** в пределах каждого VRRP instance на всех нодах

### 5. Проверка конфигурации и диагностика
```bash
# Проверка синтаксиса конфига
keepalived -t

# Проверка маршрутов
ip route show

# Проверка VIP на интерфейсах
alias ipr='ip addr show | grep -E "192.168.(87|46|45)\.(2|1)"'
ipr

# Проверка логов keepalived
journalctl -u keepalived -f --no-pager

# Проверка статуса службы
systemctl status keepalived -l --no-pager
```

### 6. Диагностика проблем с SSH подключением
Если SSH подключение не работает с ошибкой "Connection reset by peer":

```bash
# Проверка базовой connectivity
ping 192.168.45.202
traceroute 192.168.45.202
nc -zv 192.168.45.202 22

# Подробная диагностика SSH
ssh -v root@192.168.45.202

# Проверка асимметричной маршрутизации
ip route get 192.168.45.202

# Анализ VRRP трафика
tcpdump -i eth0 -n multicast
tcpdump -i eth0 -n 'host 224.0.0.18'
```

### 7. Проверка корректной работы keepalived
После настройки на всех нодах должно быть:

**На dmzgateway1 (мастер):**
```bash
    inet 192.168.87.253/24 scope global eth0
    inet 192.168.87.2/24 scope global secondary eth0      # VIP
    inet 192.168.46.253/24 scope global eth1
    inet 192.168.46.1/24 scope global secondary eth1      # VIP  
    inet 192.168.45.253/24 scope global eth2
    inet 192.168.45.1/24 scope global secondary eth2      # VIP
```

**На dmzgateway2/dmzgateway3 (бэкапы):**
```bash
    inet 192.168.87.252/24 scope global eth0              # Только физический IP
    inet 192.168.46.252/24 scope global eth1
    inet 192.168.45.252/24 scope global eth2
```

Отредактировать данный файл на остальных нодах после клонирования оригинальной машины.
<br/>



## 3. Клонируем машины
Используя GUI, может возникнуть ошибка при клонировании контейнера в ProxMox:
***unable to clone mountpoint 'mp0' (type bind) (500)***

Эта ошибка возникает при попытке клонирования контейнера LXC в Proxmox VE, когда у исходного контейнера есть привязанные (bind) точки монтирования. 

В рамках GUI можно сделать Deattach Mount Point, т.к. в нашем случае он не нужен.

**Причины проблемы:**
- Контейнер имеет bind-монтирования (тип mp0)
- Эти монтирования ссылаются на пути на хосте, которые не существуют или недоступны
- Proxmox не может автоматически обработать такие монтирования при клонировании

**Решение:**
1) Зайти на "железный" сервер, где хранится контейнер и проверить конфигурацию lxc контейнера.
```bash
root@pmx6 ~ > ccat /etc/pve/lxc/102.conf 
arch: amd64
cores: 1
features: fuse=1,mknod=1,nesting=1
hostname: dmzgateway
memory: 1024
mp0: /stg/8tb/share,mp=/shared,backup=0
net0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.87.1,hwaddr=BC:24:11:AD:EC:E2,ip=192.168.87.2/24,type=veth
net1: name=eth1,bridge=dmznet,hwaddr=BC:24:11:E6:5D:BD,ip=192.168.46.1/24,type=veth
net2: name=eth2,bridge=pgnet,firewall=1,hwaddr=BC:24:11:C4:6E:72,ip=192.168.45.1/24,type=veth
onboot: 1
ostype: debian
rootfs: ssd_1tb:102/vm-102-disk-0.raw,size=10G
swap: 0
unprivileged: 1
```
Строку временно до клонирования поменять на:
```bash
mp0: /stg/8tb/share,mp=/shared,backup=0,mountoptions=noatime
```

2) Склонировать с помощью pct.
```bash
vzdump 102
pct restore NEWID /var/lib/vz/dump/vzdump-lxc-102-*.tar.gz
```
<br/>



## 4. Установка и настройка csync2 через Ad-Hoc Ansible команды
Csync2 — это инструмент для синхронизации файлов между несколькими серверами в режиме, близком к реальному времени. Его основное предназначение — поддерживать идентичность критически важных конфигурационных файлов на всех узлах кластера.

Создаём ansible.cfg файлик:
```cfg
[defaults]
inventory = ./inventory/hosts.ini        # либо ~/.ansible/infra-proj/inventory/hosts.ini
private_key_file = ~/.ssh/id_rsa         # Путь к вашему приватному ключу
host_key_checking = False                # Отключает проверку SSH-ключей хостов
interpreter_python = /usr/bin/python3    # какой интерпретатор Python использовать на управляемых узлах.
```
### 1. Проверка подключения.
```bash
└─ ~/.ansible/infra-proj $ ansible gateways -m ping
dmzgateway2 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway1 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
dmzgateway3 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
  #-------

ansible gateways -m shell -a "nc -zv dmzgateway1 30865"
ansible gateways -m shell -a "nc -zv dmzgateway2 30865"
ansible gateways -m shell -a "nc -zv dmzgateway3 30865"
```

### 2. Добавим записи в /etc/hosts на всех узлах и проверим
```bash
ansible gateways -m blockinfile -a "path=/etc/hosts block='192.168.87.253 dmzgateway1
192.168.87.252 dmzgateway2
192.168.87.251 dmzgateway3'"
```
```bash
ansible gateways -m shell -a "ping -c 2 dmzgateway1"
ansible gateways -m shell -a "ping -c 2 dmzgateway2"
ansible gateways -m shell -a "ping -c 2 dmzgateway3"
```

### 2. Установка пакета csync2 на все шлюзы
```bash
ansible gateways -m apt -a "name=csync2 state=present update_cache=yes"
# или:
ansible gateways -i hosts -u root -k -m apt -a "name=csync2 state=present update_cache=yes"
```

### 3. Создание конфигурационных каталогов
```bash
ansible gateways -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -m file -a "path=/etc/csync2/key.d state=directory mode=0750"

# или:
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2 state=directory mode=0750"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m file -a "path=/etc/csync2/key.d state=directory mode=0750"
```

### 4.1. Генерация ключа на dmzgateway1 и копирование на остальные узлы
```bash
# Генерация ключа на dmzgateway1
ansible dmzgateway1 -i inventory/hosts.ini  -m shell -a "csync2 -k /tmp/csync2.key"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m shell -a "csync2 -k /tmp/csync2.key"

# Копирование с dmzgateway1 на локальную машину
ansible dmzgateway1 -i inventory/hosts.ini -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"
ansible dmzgateway1 -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m fetch -a "src=/tmp/csync2.key dest=./csync2.key flat=yes"

# Распространение ключа на все узлы
ansible gateways -i inventory/hosts.ini -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
ansible gateways -i ~/.ansible/infra-proj/inventory/hosts.ini -u root -k -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
```

### 4.2. Генерация ключа локальной машине (192.168.87.74) и копирование на остальные узлы
```bash
# Генерация ключа на локальной машине
csync2 -k ./csync2.key

# если Ximper Linux, то будет
sudo find /usr -name csync2
whereis csync2
/usr/sbin/csync2 -k ./csync2.key
chmod 600 ./csync2.key

# Копирование ключа на все узлы
ansible gateways -m copy -a "src=./csync2.key dest=/etc/csync2/key.d/csync2.key mode=0600"
#---------------------------------------------------
```


### 4. Создание конфигурационного файла `/etc/csync2/csync2.cfg`

```bash
ansible gateways -m copy -a "content='group cluster {
    host dmzgateway1(192.168.87.251);
    host dmzgateway2(192.168.87.252);
    host dmzgateway3(192.168.87.253);
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec /usr/sbin/service keepalived restart;
        logfile /var/log/csync2_action.log;
    }
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
Или `/etc/csync2/csync2.cfg` без SSL сертификатов
```conf
nossl * *;
group cluster {
    host dmzgateway1;
    host dmzgateway2;
    host dmzgateway3;
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    include /etc/keepalived/test_sync.txt;
    include /etc/csync2/csync2.cfg;
    include /etc/nginx/sites-enabled/*;
#    include /etc/network/interfaces.d/*;
#    action {
#        pattern /etc/keepalived/keepalived.conf;
#        exec /usr/sbin/service keepalived restart;
#        logfile /var/log/csync2_action.log;
#    }
    action {
	pattern /etc/nginx/sites-enabled/*;
	exec /usr/sbin/nginx -s reload;
        logfile /var/log/csync2_nginx.log;
    }
}
```
Создание симлинка на нужный конф. файл:
```bash
ansible gateways -m shell -a "ln -sf /etc/csync2/csync2.cfg /etc/csync2.cfg"
```
Или перемещение конфига:
```bash
ansible gateways -m shell -a "mv /etc/csync2/csync2.cfg /etc/csync2.cfg"
```


### 4 [ALT]. Создание конфигурационного файла `/etc/csync2/csync2.cfg` с помощью jinja
Создаём jinja файлик `roles/csync2/templates/csync2.cfg.j2`:
```jinja
nossl * *;
group cluster {
    host {{ ansible_hostname }};
    {% for host in groups['gateways'] %}
    {% if host != inventory_hostname %}
    host {{ host }};
    {% endif %}
    {% endfor %}

    key /etc/csync2/key.d/csync2.key;
    
    include /etc/keepalived/keepalived.conf;
    include /etc/network/interfaces.d/*;
    
    action {
        pattern /etc/keepalived/keepalived.conf;
        exec /usr/sbin/service keepalived restart;
        logfile /var/log/csync2_action.log;
    }
}
```
Команда применения этого файла:
```bash
ansible gateways -m template -a \
"src=roles/csync2/templates/csync2.cfg.j2 \
dest=/etc/csync2.cfg \
owner=root group=root mode=0644" -b
```



### 5. Создание `/etc/systemd/system/csync2.service` юнита
```bash
ansible gateways -m copy -a \
"dest=/etc/systemd/system/csync2.service \
content='[Unit]
Description=csync2 cluster synchronization
After=network.target

[Service]
Type=simple
ExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -ii -v
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target'" \
-b --diff
```
- -m copy - используем модуль для копирования файлов
- dest=/etc/systemd/system/csync2.service - путь назначения
- content='...' - содержимое файла (обратите внимание на многострочный формат)
- -b - выполнение с повышенными правами (sudo)
- --diff - покажет различия при изменении файла

т.е.
```ini
[Unit]
Description=csync2 cluster synchronization
After=network.target

[Service]
Type=simple
ExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -ii -v
Restart=on-failure
RestartSec=5s
#
#Environment="CSYNC2_SYSTEM_DIR=/etc/csync2"
#ExecStart=/usr/sbin/csync2 -D /var/lib/csync2 -v

[Install]
WantedBy=multi-user.target
```
Возможно придётся руками задать переменную CSYNC2_SYSTEM_DIR и прописать в `/etc/systemd/system/csync2.service` юнит
```
ansible gateways -m shell -a "ls -l /etc/csync2/csync2.cfg"
ansible gateways -m shell -a "echo CSYNC2_SYSTEM_DIR=$CSYNC2_SYSTEM_DIR"
```



### 6. Включение и запуск службы
```bash
ansible gateways -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -i hosts -u root -k -m service -a "name=csync2 enabled=yes state=started"
# или:
ansible gateways -m shell -a "systemctl enable csync2 --now"
ansible gateways -m shell -a "systemctl status csync2"
```

### 7. Инициализация синхронизации
```bash
ansible dmzgateway1 -m shell -a "csync2 -xv"
ansible dmzgateway1 -m shell -a "csync2 -N 192.168.87.251 -xvv"
# или:
ansible dmzgateway1 -i hosts -u root -k -m shell -a "csync2 -xv"
```

### 8. Тест синхронизации
```bash
# Создаем тестовый файл
ansible dmzgateway1 -m copy -a "content='test sync' dest=/etc/keepalived/test_sync.txt"

# Запускаем синхронизацию
ansible dmzgateway1 -m shell -a "csync2 -x"

# Проверяем на всех узлах
ansible gateways -m shell -a "ls -la /etc/keepalived/ | grep test_sync"
```

### 9. Проверка статуса
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ls -la /etc/csync2"
# или:
ansible gateways -i hosts -u root -k -m shell -a "systemctl status csync2"
```

### 10. Дополнительные проверки
```bash
# Проверка логов
ansible gateways -m shell -a "tail -n 20 /var/log/csync2*"

# Проверка подключений
ansible gateways -m shell -a "netstat -tulpn | grep csync2"
```

### 11. Создание SSL-сертификатов

#### Можно руками на мастер ноде руками создать и руками расзложить по нодам:
```bash
openssl req -x509 -newkey rsa:4096 -keyout csync2_ssl_key.pem -out csync2_ssl_cert.pem -days 3650 -nodes -subj "/CN=dmzgateway1";
# раскидываем:
for node in dmzgateway2 dmzgateway3; do
  scp /etc/csync2_ssl_{key,cert}.pem root@$node:/etc/
done
```
#### Либо попробуем развернуть сертификаты на все узлы через Ansible:
```bash
# Копируем ключ и сертификат
ansible gateways -m copy -a "src=csync2_ssl_key.pem dest=/etc/csync2_ssl_key.pem mode=0600"
ansible gateways -m copy -a "src=csync2_ssl_cert.pem dest=/etc/csync2_ssl_cert.pem mode=0644"

# Альтернативно, можно создать сертификаты прямо на узлах (если есть openssl):
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2/csync2_ssl_key.pem -out /etc/csync2/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
ansible gateways -m shell -a "chmod 600 /etc/csync2/csync2_ssl_key.pem; chmod 644 /etc/csync2/csync2_ssl_cert.pem"
```

#### Если ключи были уже, рекомендуется их удалить предварительно:
```bash
ansible gateways -m shell -a "rm -f /etc/csync2/key.d/csync2.key && csync2 -k /etc/csync2/key.d/csync2.key && chmod 600 /etc/csync2/key.d/csync2.key"
```
Исправить правила iptables:
```bash
ansible gateways -m shell -a "rm -f /etc/iptables && mkdir -p /etc/iptables"
ansible gateways -m shell -a "iptables -A INPUT -p tcp --dport 30865 -j ACCEPT && iptables-save > /etc/iptables/rules.v4"
```
#### Настроить правила iptables в директории `/etc/iptables/`
```bash
# Сначала сохраним текущие правила из /etc/iptables
ansible gateways -m shell -a "cp /etc/iptables /etc/iptables.rules.backup"

# Удалим файл /etc/iptables и создадим вместо него директорию
ansible gateways -m shell -a "rm -f /etc/iptables && mkdir -p /etc/iptables"

# Добавим правило для порта 30865 и сохраним всё в /etc/iptables/rules.v4
ansible gateways -m shell -a "iptables -A INPUT -p tcp --dport 30865 -j ACCEPT && iptables-save > /etc/iptables/rules.v4"

# Восстановим NAT-правила из бэкапа
ansible gateways -m shell -a "cat /etc/iptables.rules.backup >> /etc/iptables/rules.v4"

# Проверим, что всё сохранилось правильно
ansible gateways -m shell -a "cat /etc/iptables/rules.v4"

# Перезагрузим iptables (если нужно)
ansible gateways -m shell -a "iptables-restore < /etc/iptables/rules.v4"

# Проверим, что порт 30865 открыт
ansible gateways -m shell -a "iptables -L -n | grep 30865 || echo 'Порт не открыт'"
```

#### Ad-Hoc командой создаём сертификаты на всех узлах:
```bash
ansible gateways -m shell -a "openssl req -x509 -newkey rsa:4096 -keyout /etc/csync2_ssl_key.pem -out /etc/csync2_ssl_cert.pem -days 3650 -nodes -subj '/CN={{ inventory_hostname }}'"
```
Права:
```bash
ansible gateways -m shell -a "chmod 600 /etc/csync2_ssl_key.pem; chmod 644 /etc/csync2_ssl_cert.pem"
```


Обновляем конфигурацию **csync2**:
```conf
ansible gateways -m copy -a "content='group cluster {
    host (dmzgateway1 dmzgateway2 dmzgateway3);
    key /etc/csync2/key.d/csync2.key;
    include /etc/keepalived/keepalived.conf;
    ssl_cert /etc/csync2/csync2_ssl_cert.pem;
    ssl_key /etc/csync2/csync2_ssl_key.pem;
}' dest=/etc/csync2/csync2.cfg mode=0640"
```
```bash
ansible gateways -m shell -a "systemctl restart csync2"
ansible gateways -m shell -a "csync2 -xv"
```



```bash
ansible gateways -m shell -a "grep host /etc/csync2/csync2.cfg"
```

### 10. Настройка автоматической синхронизации (опционально)
```bash
# Синхронизация Каждые 5 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/5' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 10 минут
ansible gateways -m cron -a "name='csync2 sync' minute='*/10' job='/usr/sbin/csync2 -x'"

# Синхронизация Каждые 30 минут
ansible gateways -m cron -a "name='30-min csync2 sync' minute='*/30' job='/usr/sbin/csync2 -x'"

# Синхронизация каждый час (в 0 минут каждого часа)
ansible gateways -m cron -a "name='Hourly csync2 sync' minute='0' job='/usr/sbin/csync2 -x'"

# Синхронизация каждые 4 часа (в 0 минут каждые 4 часа)
ansible gateways -m cron -a "name='4-hour csync2 sync' minute='0' hour='*/4' job='/usr/sbin/csync2 -x'"

## Удаление cron-заданий (если нужно)
# Удалить ежечасную синхронизацию
ansible gateways -m cron -a "name='Hourly csync2 sync' state=absent"

# Удалить синхронизацию каждые 4 часа
ansible gateways -m cron -a "name='4-hour csync2 sync' state=absent"
```
Проверка cron триггеров:
```
ansible gateways -m shell -a "crontab -l | grep csync2"
```


### Ключевые особенности:
1. Все команды используют SSH-ключ (как настроено в ansible.cfg)
2. Ключ генерируется один раз локально
3. Конфигурация применяется одинаково на всех узлах
4. Не требуется ввод паролей
5. Для аутентификации используется пароль root (`-k` флаг)
6. Ключ генерируется один раз на dmzgateway1 и распространяется на остальные узлы

### Дополнительные проверки:
```bash
# Проверка синхронизированных файлов
ansible gateways -i hosts -u root -k -m shell -a "ls -la /etc/csync2"

# Проверка логов
ansible gateways -i hosts -u root -k -m shell -a "tail -n 20 /var/log/csync2*"
```

--------------------------------------------------------
### Если xinetd уже занимает порт 30865:
```bash
# Проверка порта
ansible gateways -m shell -a "netstat -tulpn | grep 30865 || ss -tulpn | grep 30865"

# Остановим xinetd (так как csync2 должен работать самостоятельно)
ansible gateways -m shell -a "systemctl stop xinetd && systemctl disable xinetd"

# Убедимся, что порт освободился
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```
```bash
# Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"

# Проверим статус
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2 || echo 'csync2 not listening'"

# Инициализируем синхронизацию
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xvv"
```

#### Вариант A: Настроим csync2 работать через xinetd
```bash
# 1. Включим xinetd обратно
ansible gateways -m shell -a "systemctl enable xinetd --now"

# 2. Настроим конфиг xinetd для csync2
ansible gateways -m copy -a "content='service csync2
{
    disable = no
    socket_type = stream
    protocol = tcp
    wait = no
    user = root
    server = /usr/sbin/csync2
    server_args = -i
    only_from = 192.168.87.0/24
}' dest=/etc/xinetd.d/csync2 mode=0640"

# 3. Перезапустим xinetd
ansible gateways -m shell -a "systemctl restart xinetd"
```

#### Вариант B: Изменим порт csync2 (если нужно оставить xinetd для других сервисов)
```bash
# В конфиге /etc/csync2/csync2.cfg добавьте:
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# Затем перезапустите:
ansible gateways -m shell -a "systemctl restart csync2"
```

#### Важно:
1. Выберите только один вариант (A или B)
2. После настройки проверьте:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (использовать xinetd), так как это стандартный подход в ALT Linux для csync2.

--------------------------------------------------------
### Если openbsd-inetd (простой inetd) уже занимает порт 30865:
### 1. Остановим inetd и освободим порт 30865:
```bash
ansible gateways -m shell -a "systemctl stop openbsd-inetd && systemctl disable openbsd-inetd"
```

### 2. Проверим освобождение порта:
```bash
ansible gateways -m shell -a "ss -tulpn | grep 30865 || echo 'Port 30865 is free'"
```

### 3. Перезапустим csync2:
```bash
ansible gateways -m shell -a "systemctl restart csync2"
```

### 4. Проверим работу csync2:
```bash
ansible gateways -m shell -a "systemctl status csync2"
ansible gateways -m shell -a "ss -tulpn | grep csync2"
```

### 5. Инициализируем синхронизацию:
```bash
ansible dmzgateway1 -m shell -a "/usr/sbin/csync2 -xv"  # как бы на одной ноде делается
ansible gateways -m shell -a "/usr/sbin/csync2 -xv"
```

### Если нужно оставить inetd для других сервисов:

#### Вариант A: Настроим csync2 через inetd
```bash
# 1. Добавим конфиг для inetd
ansible gateways -m copy -a "content='30865 stream tcp nowait root /usr/sbin/csync2 csync2 -i' dest=/etc/inetd.conf mode=0640"

# 2. Перезапустим inetd
ansible gateways -m shell -a "systemctl restart openbsd-inetd"
```

#### Вариант B: Изменим порт csync2
```bash
# 1. В конфиге csync2 добавим порт
ansible gateways -m lineinfile -a "path=/etc/csync2/csync2.cfg line='    port 30866;' insertafter='group cluster'"

# 2. Перезапустим csync2
ansible gateways -m shell -a "systemctl restart csync2"
```

### Проверка после настройки:
```bash
ansible gateways -m shell -a "csync2 -T"  # Проверка конфигурации
ansible gateways -m shell -a "csync2 -xv" # Тест синхронизации
```
Рекомендую **Вариант A** (настройка через inetd), так как это стандартный подход в системах с openbsd-inetd. Это обеспечит правильную работу csync2 без конфликтов портов.

--------------------------------------------------------
